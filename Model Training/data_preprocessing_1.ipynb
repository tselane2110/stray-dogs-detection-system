{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tselane2110/stray-dogs-detection-system/blob/main/Model%20Training/data_preprocessing_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReadMe:  (basic introduction of our code)\n",
        "\n",
        "* The actual dataset that I used is from kaggle : https://www.kaggle.com/datasets/jessicali9530/stanford-dogs-dataset\n",
        "\n",
        "* It has 120 different folders for each kind of dog, with 20k images in total.\n",
        "\n",
        "* We did data-cleaining as in deleting the folders of the dog-breeds which I did not need. So final number of folders was 40. With almost 7k+ something total images.\n",
        "\n",
        "* We then converted the annotations into yolov5 format. The total number of images then reduced to 6876.\n",
        "\n",
        "* Then we splitted our dataset into train, validation and test.\n",
        "\n",
        "* We then chunked out train-data because it was too much for Colab to handle (code was crashing), into 5 chunks, each of almost 1100 images approximately.\n",
        "\n",
        "* For training, I trained the 1st chunk, it's best weight was then used for the training of the 2nd chunk, and the same for repeated for the other 2 remaining chunks.\n",
        "\n",
        "* The final best weight (of chunk4) was used for deploying the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "ViQXtqEt5X9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preparing data:"
      ],
      "metadata": {
        "id": "q7o5NyMh8BoN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui2hdJw0W_V0"
      },
      "source": [
        "* upload the fypdataset.zip and annotations.zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlPO-yu5ka9I"
      },
      "outputs": [],
      "source": [
        "!unzip /content/fypdataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZa2mnpLXEc4"
      },
      "source": [
        "* functions to convert annotation files from xml to txt (yolov5 version)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import pickle\n",
        "import xml.etree.ElementTree as ET\n",
        "from os import listdir, getcwd\n",
        "from os.path import join\n",
        "\n",
        "dirs = ['fypdataset']\n",
        "classes = [\n",
        "           \"Chihuahua\", \"toy_terrier\", \"Rhodesian_ridgeback\", \"basset\", \"beagle\", \"bloodhound\", \"bluetick\", \"black-and-tan_coonhound\", \"Walker_hound\", \"English_foxhound\", \"redbone\", \"Italian_greyhound\", \"whippet\", \"Ibizan_hound\", \"Saluki\", \"Weimaraner\", \"Staffordshire_bullterrier\", \"American_Staffordshire_terrier\", \"golden_retriever\", \"Labrador_retriever\", \"Chesapeake_Bay_retriever\", \"German_short-haired_pointer\", \"Brittany_spaniel\", \"kuvasz\", \"schipperke\", \"malinois\", \"kelpie\", \"Rottweiler\", \"miniature_pinscher\", \"EntleBucher\", \"boxer\", \"Great_Dane\", \"Saint_Bernard\", \"Eskimo_dog\", \"basenji\", \"Leonberg\", \"Pembroke\", \"Cardigan\", \"Mexican_hairless\", \"dingo\", \"African_hunting_dog\"       ]\n",
        "\n",
        "def getImagesInDir(dir_path):\n",
        "    image_list = []\n",
        "    for filename in os.listdir(dir_path+\"/images\"):\n",
        "      image_list.append(filename)\n",
        "\n",
        "    return image_list\n",
        "\n",
        "def convert(size, box):\n",
        "    dw = 1./(size[0])\n",
        "    dh = 1./(size[1])\n",
        "    x = (box[0] + box[1])/2.0 - 1\n",
        "    y = (box[2] + box[3])/2.0 - 1\n",
        "    w = box[1] - box[0]\n",
        "    h = box[3] - box[2]\n",
        "    x = x*dw\n",
        "    w = w*dw\n",
        "    y = y*dh\n",
        "    h = h*dh\n",
        "    return (x,y,w,h)\n",
        "\n",
        "def convert_annotation(dir_path, output_path, image_path):\n",
        "    basename = os.path.basename(image_path)\n",
        "    basename_no_ext = os.path.splitext(basename)[0]\n",
        "    # image_path is like n02085620_10074.jpg\n",
        "    # basename_no_ext is like n02085620_10074\n",
        "    # dir_path is /content/fypdataset/\n",
        "    # output_path is /content/fypdataset/yolo\n",
        "    # in_file should be: /content/fypdataset/labels/n02085620_10074\n",
        "    in_file = open(dir_path + '/labels/' + basename_no_ext)\n",
        "    out_file = open(output_path + \"/\" + basename_no_ext + '.txt', 'w')\n",
        "    tree = ET.parse(in_file)\n",
        "    root = tree.getroot()\n",
        "    size = root.find('size')\n",
        "    w = int(size.find('width').text)\n",
        "    h = int(size.find('height').text)\n",
        "\n",
        "    for obj in root.iter('object'):\n",
        "        difficult = obj.find('difficult').text\n",
        "        cls = obj.find('name').text\n",
        "        if cls not in classes or int(difficult)==1:\n",
        "            continue\n",
        "        cls_id = 0\n",
        "        xmlbox = obj.find('bndbox')\n",
        "        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text),\n",
        "             float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))\n",
        "        bb = convert((w,h), b)\n",
        "        out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n')\n",
        "\n",
        "cwd = getcwd()\n",
        "\n",
        "for dir_path in dirs:\n",
        "    full_dir_path = cwd + '/' + dir_path\n",
        "    print(full_dir_path)\n",
        "    output_path = full_dir_path +'/yolo'\n",
        "    print(output_path)\n",
        "\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "        print(\"output path created!\")\n",
        "\n",
        "    image_paths = getImagesInDir(full_dir_path)\n",
        "    list_file = open(full_dir_path + '.txt', 'w')\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        list_file.write(image_path + '\\n')\n",
        "        convert_annotation(full_dir_path, output_path, image_path)\n",
        "    list_file.close()\n",
        "\n",
        "    print(\"Finished processing: \" + dir_path)"
      ],
      "metadata": {
        "id": "HqPeehgbtdoo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2750d2-4523-4dc1-b4f0-908351128439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fypdataset\n",
            "/content/fypdataset/yolo\n",
            "Finished processing: fypdataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* train, val, test split:"
      ],
      "metadata": {
        "id": "-SCw9xA_67k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code to confirm my logic XD\n",
        "\n",
        "list=[1,2,3]\n",
        "\n",
        "for i in list:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ueuM2HPGj9v",
        "outputId": "9a622741-c87b-4c0e-b063-ef4adf879b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiTue97jV6o8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3de8ded-264c-4f19-b427-96ca10833074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no of images:  6876\n",
            "no of labels:  6876\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "# split the image data into train, val, and test sets\n",
        "\n",
        "path_to_annotation_files = []\n",
        "path_to_image_files = []\n",
        "annotation_root= \"/content/fypdataset/yolo\"\n",
        "root_path = \"/content/fypdataset\"\n",
        "for class_id in os.listdir(annotation_root):\n",
        "  annotation_path = os.path.join(annotation_root, class_id)       #/content/fypdataset/yolo/n02085620_1298.txt\n",
        "  text=class_id                         # we need i == n02110806_3971.jpg\n",
        "  d=re.search ('\\d+_\\d+', text)\n",
        "  if d.group(0)!=None:\n",
        "    b=d.group(0)\n",
        "    x=\"n\"+b+\".jpg\"     #x= n02085620_1298.jpg\n",
        "  image_path = os.path.join(root_path, \"images/\"+x)   #/content/fypdataset/images/n02085620_1298.jpg\n",
        "  path_to_annotation_files.append(annotation_path)\n",
        "  path_to_image_files.append(image_path)\n",
        "\n",
        "\n",
        "\n",
        "print(\"no of images: \", len(path_to_annotation_files))\n",
        "print(\"no of labels: \", len(path_to_image_files))\n",
        "\n",
        "train_images, val_images, train_annotations, val_annotations= train_test_split(path_to_image_files, path_to_annotation_files,\n",
        "                                                                               test_size=0.2, random_state=1)\n",
        "\n",
        "val_images, test_images, val_annotations, test_annotations=train_test_split(val_images, val_annotations,\n",
        "                                                                           test_size=0.5, random_state=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_images) #/content/fypdataset/images/n02085620_10074.jpg\n",
        "print(train_annotations) #/content/fypdataset/yolo/n02085620_1073.txt"
      ],
      "metadata": {
        "id": "8Jqq1bUJTSic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B11Ga5JRoxkv"
      },
      "source": [
        "# moving the train, test and val split files into their respective folders:\n",
        "\n",
        "* I created 2 new folders as images1 and labels1 in fypdataset folder, and then I created training, testing and validation folders within each of them. So that I can delete the old images and yolo folder (I also deleted the initial labels/annotations folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNu7NsT5HxFP"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "def moves_files_to_folder(list_of_files, destination_folder):\n",
        "  for f in list_of_files:\n",
        "    try:\n",
        "      shutil.copy(f, destination_folder)\n",
        "    except:\n",
        "      print(f)\n",
        "      assert False\n",
        "\n",
        "moves_files_to_folder(train_images, \"/content/fypdataset/images1/training\")\n",
        "moves_files_to_folder(val_images, \"/content/fypdataset/images1/validation\")\n",
        "moves_files_to_folder(test_images, \"/content/fypdataset/images1/testing\")\n",
        "moves_files_to_folder(train_annotations, \"/content/fypdataset/labels1/training\")\n",
        "moves_files_to_folder(val_annotations, \"/content/fypdataset/labels1/validation\")\n",
        "moves_files_to_folder(test_annotations, \"/content/fypdataset/labels1/testing\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now deleting the old images and yolo folder:"
      ],
      "metadata": {
        "id": "Q3VNnSwfeV1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.rmtree(\"/content/fypdataset/images\")\n",
        "shutil.rmtree(\"/content/fypdataset/yolo\")"
      ],
      "metadata": {
        "id": "S5hnBhRbeaPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chunking data into multiple training files for comparatively quick training:\n",
        "\n",
        "\n",
        "* create 5 folders as training1, training2, training3, training4 and training5 in both, images and labels folder.\n",
        "\n",
        "**NOTE: I chunked my data into multiple folders because back then, I did not know that I can just use FREE GPU and give the whole dataset for training in one go!**"
      ],
      "metadata": {
        "id": "EYBqmOYECRFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing dataset again: (if you reconnected your runtime/ reuploaded your fypdataset.zip)\n",
        "\n",
        "!unzip /content/fypdataset.zip\n"
      ],
      "metadata": {
        "id": "yQe35Z9iLsav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for image training data:\n",
        "import os\n",
        "\n",
        "path= \"/content/content/fypdataset/images/training\"\n",
        "j=0\n",
        "\n",
        "chunki1=[]\n",
        "chunki2=[]\n",
        "chunki3=[]\n",
        "chunki4=[]\n",
        "chunki5=[]\n",
        "\n",
        "\n",
        "for i in os.listdir(path):\n",
        "  if j>=0 and j<1100:\n",
        "    chunki1.append(i)\n",
        "\n",
        "  if j>=1100 and j<2200:\n",
        "    chunki2.append(i)\n",
        "\n",
        "  if j>=2200 and j<3300:\n",
        "    chunki3.append(i)\n",
        "\n",
        "  if j>=3300 and j<4400:\n",
        "    chunki4.append(i)\n",
        "\n",
        "  if j>=4400 and j<5500:\n",
        "    chunki5.append(i)\n",
        "  j+=1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5i0XkQ9XCQVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"chunk1: \\n\", chunki1)\n",
        "print(\"chunk2: \\n\", chunki2)\n",
        "print(\"chunk3: \\n\", chunki3)\n",
        "print(\"chunk4: \\n\", chunki4)\n",
        "print(\"chunk5: \\n\", chunki5)"
      ],
      "metadata": {
        "id": "AqPO4bwOMynQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now divide the data into content/fypdataset/images/training1, training2, training3, training4\n",
        "#newchunki1=[]\n",
        "newchunki2=[]\n",
        "newchunki3=[]\n",
        "newchunki4=[]\n",
        "newchunki5=[]\n",
        "\n",
        "# for i in chunki1:\n",
        "#   newchunki1.append(\"/content/fypdataset/images/training/\"+i)\n",
        "\n",
        "for i in chunki2:\n",
        "  newchunki2.append(\"/content/content/fypdataset/images/training/\"+i)\n",
        "\n",
        "for i in chunki3:\n",
        "  newchunki3.append(\"/content/content/fypdataset/images/training/\"+i)\n",
        "\n",
        "for i in chunki4:\n",
        "  newchunki4.append(\"/content/content/fypdataset/images/training/\"+i)\n",
        "\n",
        "for i in chunki5:\n",
        "  newchunki5.append(\"/content/content/fypdataset/images/training/\"+i)\n",
        "\n",
        "\n",
        "# moves_files_to_folder(newchunki1, \"/content/fypdataset/images/training1\")\n",
        "moves_files_to_folder(newchunki2, \"/content/content/fypdataset/images/training2\")\n",
        "moves_files_to_folder(newchunki3, \"/content/content/fypdataset/images/training3\")\n",
        "moves_files_to_folder(newchunki4, \"/content/content/fypdataset/images/training4\")\n",
        "moves_files_to_folder(newchunki5, \"/content/content/fypdataset/images/training5\")\n"
      ],
      "metadata": {
        "id": "Owd28_vbYH5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for labels training data:\n",
        "\n",
        "\n",
        "#chunkl1=[]\n",
        "chunkl2=[]\n",
        "chunkl3=[]\n",
        "chunkl4=[]\n",
        "chunkl5=[]\n",
        "\n",
        "# for i in chunki1:\n",
        "#   base = os.path.splitext(i)[0]\n",
        "#   chunkl1.append(base+\".txt\")\n",
        "\n",
        "for i in chunki2:\n",
        "  base = os.path.splitext(i)[0]\n",
        "  chunkl2.append(base+\".txt\")\n",
        "\n",
        "for i in chunki3:\n",
        "  base = os.path.splitext(i)[0]\n",
        "  chunkl3.append(base+\".txt\")\n",
        "\n",
        "for i in chunki4:\n",
        "  base = os.path.splitext(i)[0]\n",
        "  chunkl4.append(base+\".txt\")\n",
        "\n",
        "for i in chunki5:\n",
        "  base = os.path.splitext(i)[0]\n",
        "  chunkl5.append(base+\".txt\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FhFkVJqDCQe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"chunk1: \\n\", chunkl1)\n",
        "print(\"chunk2: \\n\", chunkl2)\n",
        "print(\"chunk3: \\n\", chunkl3)\n",
        "print(\"chunk4: \\n\", chunkl4)\n",
        "print(\"chunk5: \\n\", chunkl5)"
      ],
      "metadata": {
        "id": "jwsA0gTyMNNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now divide the data into content/fypdataset/labels/training1, training2, training3, training4\n",
        "\n",
        "#newchunkl1=[]\n",
        "newchunkl2=[]\n",
        "newchunkl3=[]\n",
        "newchunkl4=[]\n",
        "newchunkl5=[]\n",
        "\n",
        "# for i in chunkl1:\n",
        "#   newchunkl1.append(\"/content/fypdataset/labels/training/\"+i)\n",
        "\n",
        "\n",
        "for i in chunkl2:\n",
        "  newchunkl2.append(\"/content/content/fypdataset/labels/training/\"+i)\n",
        "\n",
        "\n",
        "for i in chunkl3:\n",
        "  newchunkl3.append(\"/content/content/fypdataset/labels/training/\"+i)\n",
        "\n",
        "\n",
        "for i in chunkl4:\n",
        "  newchunkl4.append(\"/content/content/fypdataset/labels/training/\"+i)\n",
        "\n",
        "\n",
        "for i in chunkl5:\n",
        "  newchunkl5.append(\"/content/content/fypdataset/labels/training/\"+i)\n",
        "\n",
        "\n",
        "# moves_files_to_folder(newchunkl1, \"/content/fypdataset/labels/training1\")\n",
        "moves_files_to_folder(newchunkl2, \"/content/content/fypdataset/labels/training2\")\n",
        "moves_files_to_folder(newchunkl3, \"/content/content/fypdataset/labels/training3\")\n",
        "moves_files_to_folder(newchunkl4, \"/content/content/fypdataset/labels/training4\")\n",
        "moves_files_to_folder(newchunkl5, \"/content/content/fypdataset/labels/training5\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HnXmtOeJYNBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# now deleting the training folder from images and labels folder:\n"
      ],
      "metadata": {
        "id": "1tgbdgaAipv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.rmtree(\"/content/content/fypdataset/images/training\")\n",
        "shutil.rmtree(\"/content/content/fypdataset/images/training\")"
      ],
      "metadata": {
        "id": "nGaDuomEioRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# converting fypdataset into zipped file,, and downloading it, so that we dont lose our data.\n"
      ],
      "metadata": {
        "id": "Q5cmxxve5vwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!zip /content/content/fypdataset.zip -r /content/content/fypdataset\n",
        "# from google.colab import files\n",
        "# files.download(\"/content/fypdataset.zip\")\n",
        "\n",
        "# you can also just double-click on the zip file as shown in the files tab"
      ],
      "metadata": {
        "id": "2VcbyZqqwekj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Working on the object detection model :"
      ],
      "metadata": {
        "id": "n28iv36u6UtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# incase you had to reconnect the runtime, or you reuploaded the zip file\n",
        "!unzip /content/fypdataset.zip"
      ],
      "metadata": {
        "id": "oyJ5rgrK_yHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* cloning with yolov5 repository"
      ],
      "metadata": {
        "id": "XnbWmBC-9RGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n"
      ],
      "metadata": {
        "id": "zRc8FpLP6azJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa55f412-cb3a-46b8-8b18-a5348a380a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 12773, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 12773 (delta 29), reused 40 (delta 22), pack-reused 12719\u001b[K\n",
            "Receiving objects: 100% (12773/12773), 12.34 MiB | 13.78 MiB/s, done.\n",
            "Resolving deltas: 100% (8786/8786), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* changing directory to yolov5"
      ],
      "metadata": {
        "id": "xtJN52QN9Xya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5"
      ],
      "metadata": {
        "id": "VbC9xigD9Wdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8017c8a3-d837-476e-a899-ce96964219b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* installing requirements for yolov5"
      ],
      "metadata": {
        "id": "rt7igRQ59keI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "2rp_wxTa9hcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training our model :"
      ],
      "metadata": {
        "id": "UtPOOth1QXB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. we will first train our chunk 1, and it's output best weight will be input to the weights for training chunk2, and then its output best weight will be input to chunk3, and the same thing will be followed for chunk4.\n",
        "\n",
        "\n",
        "* Training chunk1:"
      ],
      "metadata": {
        "id": "auYRXA513115"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 12 --epochs 10  --data /content/yolov5/data/custom_dataset.yaml --weights yolov5s.pt --cache"
      ],
      "metadata": {
        "id": "i70WPQUOQWIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da2fddc5-a479-4e9b-d2e1-7d8646fee3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/yolov5/data/custom_dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=12, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v6.1-304-g51fb467 Python-3.7.13 torch-1.11.0+cu102 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 270 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.00046875\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/fypdataset/labels/training1.cache' images and labels... 1100 found, 0 missing, 0 empty, 0 corrupt: 100% 1100/1100 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB ram): 100% 1100/1100 [00:06<00:00, 182.86it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/fypdataset/labels/validation.cache' images and labels... 688 found, 0 missing, 0 empty, 0 corrupt: 100% 688/688 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.6GB ram): 100% 688/688 [00:04<00:00, 157.14it/s]\n",
            "Plotting labels to runs/train/exp2/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.52 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/9        0G   0.06938   0.02813         0        19       640: 100% 92/92 [25:03<00:00, 16.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:28<00:00,  9.26s/it]\n",
            "                 all        688        749      0.598      0.765      0.651      0.294\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/9        0G   0.04418   0.01931         0        21       640: 100% 92/92 [25:01<00:00, 16.32s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:12<00:00,  8.69s/it]\n",
            "                 all        688        749      0.574      0.721      0.673      0.372\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/9        0G   0.04264    0.0181         0        23       640: 100% 92/92 [25:13<00:00, 16.46s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:07<00:00,  8.53s/it]\n",
            "                 all        688        749       0.71      0.728      0.776      0.359\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/9        0G    0.0358   0.01872         0        14       640: 100% 92/92 [25:21<00:00, 16.54s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:07<00:00,  8.52s/it]\n",
            "                 all        688        749      0.804      0.767      0.822      0.421\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/9        0G   0.03486   0.01834         0        21       640: 100% 92/92 [25:25<00:00, 16.58s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:06<00:00,  8.50s/it]\n",
            "                 all        688        749      0.884      0.857      0.908      0.522\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/9        0G   0.03151   0.01749         0        22       640: 100% 92/92 [25:17<00:00, 16.50s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:11<00:00,  8.68s/it]\n",
            "                 all        688        749       0.89      0.878      0.912      0.534\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/9        0G    0.0302   0.01769         0        18       640: 100% 92/92 [25:21<00:00, 16.54s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:08<00:00,  8.56s/it]\n",
            "                 all        688        749      0.908      0.868      0.916      0.552\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       7/9        0G   0.02785   0.01691         0        25       640: 100% 92/92 [25:40<00:00, 16.75s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:15<00:00,  8.81s/it]\n",
            "                 all        688        749      0.908      0.909      0.941      0.606\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       8/9        0G   0.02484   0.01593         0        29       640: 100% 92/92 [25:27<00:00, 16.60s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:10<00:00,  8.64s/it]\n",
            "                 all        688        749      0.899      0.904      0.936      0.626\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       9/9        0G   0.02268   0.01601         0        24       640: 100% 92/92 [25:25<00:00, 16.58s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:08<00:00,  8.57s/it]\n",
            "                 all        688        749      0.913      0.921       0.94      0.653\n",
            "\n",
            "10 epochs completed in 4.923 hours.\n",
            "Optimizer stripped from runs/train/exp2/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp2/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp2/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:54<00:00,  8.09s/it]\n",
            "                 all        688        749      0.915       0.92       0.94      0.652\n",
            "Results saved to \u001b[1mruns/train/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python val.py --weights /content/yolov5/runs/train/exp2/weights/best.pt --data /content/yolov5/data/custom_dataset.yaml --img 640\n",
        "# for testing"
      ],
      "metadata": {
        "id": "60iJ2oDECJbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/yolov5/runs/train/exp2/weights/best.pt --img 640 --conf 0.5 --source \"/content/fypdataset/images/testing\"\n",
        "# for inference\n",
        "# detecting new images"
      ],
      "metadata": {
        "id": "23nibKqFuskN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# which folders to delete:"
      ],
      "metadata": {
        "id": "J2m9Z66EKYwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree(\"/content/yolov5/runs/train/exp\")\n",
        "#delete exp 3 4 and 5 from train\n",
        "# exp2 from detect\n",
        "# exp, exp2 and exp4 from val"
      ],
      "metadata": {
        "id": "O6jUzUmRJ4Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "downloaded the files to continue work later on..."
      ],
      "metadata": {
        "id": "TkVtMUNL4PQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -r /content/fypdataset.zip /content/fypdataset"
      ],
      "metadata": {
        "id": "SdgT2HglCPmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/yolov5.zip /content/yolov5"
      ],
      "metadata": {
        "id": "THuwUQSw4CIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.download(\"/content/fypdataset.zip\")\n",
        "# files.download(\"/content/fypdataset(chunked)\")"
      ],
      "metadata": {
        "id": "MMXkr0kKCQR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload:\n",
        "\n",
        "1. Fypdataset (the chunked one)\n",
        "2. best.pt (of trained chunk1) ----- [to train chunk2]\n",
        "\n",
        "* you'll have to create the custom_dataset.yaml file in the data folder of yolov5, everytime when you upload your dataset and when you clone the yolov5 repository :))\n"
      ],
      "metadata": {
        "id": "n4zOelc74csF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/fypdataset.zip"
      ],
      "metadata": {
        "id": "2H9T44KgV3ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* cloning yolov5:"
      ],
      "metadata": {
        "id": "PzjhF0U7BK82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "CXvZ6USzBB43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Training chunk2:\n",
        "\n",
        "--> Update the train file path in the custom_dataset.yaml file for the 2nd chunk"
      ],
      "metadata": {
        "id": "P-zmDdJdA7Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 12 --epochs 10  --data /content/yolov5/data/custom_dataset.yaml --weights /content/best.pt --cache"
      ],
      "metadata": {
        "id": "e3zn9GQ7AhWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63798412-0239-4eec-ac73-d72df4b45ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/best.pt, cfg=, data=/content/yolov5/data/custom_dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=12, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v6.1-306-gfbe67e4 Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 125MB/s]\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 270 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 349/349 items from /content/best.pt\n",
            "Scaled weight_decay = 0.00046875\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/content/fypdataset/labels/training2' images and labels...1100 found, 0 missing, 0 empty, 0 corrupt: 100% 1100/1100 [00:00<00:00, 1732.68it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/content/fypdataset/labels/training2.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB ram): 100% 1100/1100 [00:05<00:00, 196.98it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/content/fypdataset/labels/validation' images and labels...688 found, 0 missing, 0 empty, 0 corrupt: 100% 688/688 [00:00<00:00, 911.45it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/content/fypdataset/labels/validation.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.6GB ram): 100% 688/688 [00:03<00:00, 187.34it/s]\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.53 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/9        0G   0.02665   0.01723         0        23       640: 100% 92/92 [21:48<00:00, 14.22s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:36<00:00,  7.47s/it]\n",
            "                 all        688        749      0.907      0.919      0.939      0.653\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/9        0G   0.02457   0.01699         0        23       640: 100% 92/92 [21:34<00:00, 14.07s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:35<00:00,  7.41s/it]\n",
            "                 all        688        749      0.919      0.909      0.947      0.643\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/9        0G   0.02971   0.01672         0        24       640: 100% 92/92 [21:34<00:00, 14.07s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:35<00:00,  7.43s/it]\n",
            "                 all        688        749      0.902       0.92      0.945      0.622\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/9        0G   0.03115   0.01665         0        13       640: 100% 92/92 [21:30<00:00, 14.03s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:34<00:00,  7.41s/it]\n",
            "                 all        688        749      0.907      0.891       0.94      0.613\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/9        0G   0.03002   0.01674         0        20       640: 100% 92/92 [21:32<00:00, 14.05s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:34<00:00,  7.38s/it]\n",
            "                 all        688        749      0.903      0.923      0.945      0.629\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/9        0G   0.02759   0.01611         0        22       640: 100% 92/92 [21:34<00:00, 14.07s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:34<00:00,  7.41s/it]\n",
            "                 all        688        749      0.915      0.916      0.948       0.65\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/9        0G    0.0278   0.01649         0        17       640: 100% 92/92 [21:30<00:00, 14.03s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:34<00:00,  7.39s/it]\n",
            "                 all        688        749      0.934      0.912      0.956      0.673\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       7/9        0G   0.02293   0.01532         0        27       640: 100% 92/92 [21:30<00:00, 14.03s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:33<00:00,  7.37s/it]\n",
            "                 all        688        749      0.929      0.923      0.955       0.69\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       8/9        0G    0.0234   0.01547         0        31       640: 100% 92/92 [21:30<00:00, 14.03s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:35<00:00,  7.42s/it]\n",
            "                 all        688        749      0.932       0.92      0.957      0.708\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       9/9        0G   0.02198   0.01529         0        27       640: 100% 92/92 [21:30<00:00, 14.02s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:35<00:00,  7.42s/it]\n",
            "                 all        688        749       0.95      0.924      0.965      0.721\n",
            "\n",
            "10 epochs completed in 4.192 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:22<00:00,  6.99s/it]\n",
            "                 all        688        749      0.952      0.919      0.965      0.722\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python val.py --weights /content/best.pt --data /content/yolov5/data/custom_dataset.yaml --img 640\n"
      ],
      "metadata": {
        "id": "v6jBiT8Gywgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2422af9-52dc-4855-e15a-337ad663a4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/custom_dataset.yaml, weights=['/content/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 ðŸš€ v6.1-306-gfbe67e4 Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/content/fypdataset/labels/validation.cache' images and labels... 688 found, 0 missing, 0 empty, 0 corrupt: 100% 688/688 [00:00<?, ?it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 22/22 [03:31<00:00,  9.64s/it]\n",
            "                 all        688        749      0.914      0.924      0.941      0.652\n",
            "Speed: 2.3ms pre-process, 302.5ms inference, 0.5ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/dogs.zip"
      ],
      "metadata": {
        "id": "PCgO7G-ddCAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --img 640 --conf 0.5 --source \"/content/yolov5/dogs\"\n",
        "# for inference\n",
        "# detecting new images"
      ],
      "metadata": {
        "id": "mRS0J28SzhkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --img 640 --conf 0.5 --source \"/content/content/fypdataset/images/testing\"\n",
        "# for inference\n",
        "# detecting new images"
      ],
      "metadata": {
        "id": "623MSjN2dgYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/content.zip -r /content/content"
      ],
      "metadata": {
        "id": "LoUvxINkd0Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/yolov5.zip -r /content/yolov5"
      ],
      "metadata": {
        "id": "-3oG-sTod4cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Training chunk3:"
      ],
      "metadata": {
        "id": "1ajcnVkZgm_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 12 --epochs 10  --data /content/yolov5/data/custom_dataset.yaml --weights /content/yolov5/runs/train/exp/weights/best.pt --cache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJl5P_uPgmLr",
        "outputId": "1479a17a-c401-46c0-9002-99f4415199d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/yolov5/runs/train/exp/weights/best.pt, cfg=, data=/content/yolov5/data/custom_dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=12, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n",
            "From https://github.com/ultralytics/yolov5\n",
            "   2d34408..1ad285a  classifier -> origin/classifier\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v6.1-306-gfbe67e4 Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 270 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 349/349 items from /content/yolov5/runs/train/exp/weights/best.pt\n",
            "Scaled weight_decay = 0.00046875\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/content/fypdataset/labels/training3' images and labels...1100 found, 0 missing, 0 empty, 0 corrupt: 100% 1100/1100 [00:00<00:00, 1904.83it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/content/fypdataset/labels/training3.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB ram): 100% 1100/1100 [00:06<00:00, 182.13it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/content/fypdataset/labels/validation.cache' images and labels... 688 found, 0 missing, 0 empty, 0 corrupt: 100% 688/688 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.6GB ram): 100% 688/688 [00:04<00:00, 167.45it/s]\n",
            "Plotting labels to runs/train/exp2/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.54 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/9        0G   0.02263    0.0153         0        21       640: 100% 92/92 [22:07<00:00, 14.43s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:41<00:00,  7.64s/it]\n",
            "                 all        688        749      0.954      0.928      0.966      0.728\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/9        0G   0.02246   0.01545         0        28       640: 100% 92/92 [22:00<00:00, 14.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:40<00:00,  7.61s/it]\n",
            "                 all        688        749      0.939       0.94      0.964      0.709\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/9        0G   0.02487   0.01537         0        24       640: 100% 92/92 [21:53<00:00, 14.27s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:38<00:00,  7.53s/it]\n",
            "                 all        688        749      0.946      0.927      0.956      0.668\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/9        0G   0.02415    0.0149         0        15       640: 100% 92/92 [21:50<00:00, 14.25s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:38<00:00,  7.52s/it]\n",
            "                 all        688        749      0.929      0.921      0.951      0.682\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/9        0G   0.02531   0.01532         0        26       640: 100% 92/92 [21:58<00:00, 14.33s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:39<00:00,  7.56s/it]\n",
            "                 all        688        749      0.917      0.917      0.949      0.658\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/9        0G   0.02271   0.01482         0        23       640: 100% 92/92 [22:04<00:00, 14.40s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:39<00:00,  7.57s/it]\n",
            "                 all        688        749      0.937      0.924       0.96      0.695\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/9        0G   0.02236   0.01516         0        16       640: 100% 92/92 [22:08<00:00, 14.44s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:40<00:00,  7.60s/it]\n",
            "                 all        688        749      0.915      0.922      0.948      0.684\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       7/9        0G   0.02095   0.01464         0        26       640: 100% 92/92 [22:13<00:00, 14.49s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:42<00:00,  7.68s/it]\n",
            "                 all        688        749      0.939      0.924       0.96      0.715\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       8/9        0G   0.02014   0.01414         0        28       640: 100% 92/92 [22:13<00:00, 14.49s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:46<00:00,  7.83s/it]\n",
            "                 all        688        749      0.941      0.939      0.965      0.732\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       9/9        0G   0.02115   0.01464         0        28       640: 100% 92/92 [22:25<00:00, 14.63s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:44<00:00,  7.73s/it]\n",
            "                 all        688        749      0.944      0.946      0.965      0.733\n",
            "\n",
            "10 epochs completed in 4.298 hours.\n",
            "Optimizer stripped from runs/train/exp2/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp2/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp2/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:29<00:00,  7.23s/it]\n",
            "                 all        688        749      0.944      0.944      0.965      0.733\n",
            "Results saved to \u001b[1mruns/train/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python val.py --weights /content/best.pt --data /content/yolov5/custom_dataset.yaml --img 640\n"
      ],
      "metadata": {
        "id": "e7fqTWJkSWLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1331593-f3db-472c-8a08-33538a5be7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/custom_dataset.yaml, weights=['/content/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 ðŸš€ v6.1-306-gfbe67e4 Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 14.6MB/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/fypdataset/labels/validation' images and labels...688 found, 0 missing, 0 empty, 0 corrupt: 100% 688/688 [00:00<00:00, 1538.33it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/fypdataset/labels/validation.cache\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 22/22 [04:11<00:00, 11.45s/it]\n",
            "                 all        688        749      0.949      0.945      0.967      0.736\n",
            "Speed: 2.5ms pre-process, 359.5ms inference, 0.7ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/best.pt --img 640 --conf 0.5 --source /content/fypdataset/images/testing\n"
      ],
      "metadata": {
        "id": "KlHl3EBUkxYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/yolov5/runs/val.zip -r /content/yolov5/runs/val\n",
        "!zip /content/yolov5/runs/detect.zip -r /content/yolov5/runs/detect"
      ],
      "metadata": {
        "id": "2j0i2AE_oSfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Training chunk3:"
      ],
      "metadata": {
        "id": "pPVrRyrnt6ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 12 --epochs 10  --data /content/yolov5/data/custom_dataset.yaml --weights /content/best.pt --cache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coUZaFeJt6VW",
        "outputId": "c1ff35a4-4849-425b-8a39-44900d87c6e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/best.pt, cfg=, data=/content/yolov5/data/custom_dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=12, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 6 (delta 4), reused 6 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), done.\n",
            "From https://github.com/ultralytics/yolov5\n",
            "   c526341..62cceed  classifier -> origin/classifier\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v6.1-306-gfbe67e4 Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 270 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 349/349 items from /content/best.pt\n",
            "Scaled weight_decay = 0.00046875\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/fypdataset/labels/training4' images and labels...1100 found, 0 missing, 0 empty, 0 corrupt: 100% 1100/1100 [00:01<00:00, 904.17it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/fypdataset/labels/training4.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB ram): 100% 1100/1100 [00:06<00:00, 161.68it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/fypdataset/labels/validation.cache' images and labels... 688 found, 0 missing, 0 empty, 0 corrupt: 100% 688/688 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.6GB ram): 100% 688/688 [00:04<00:00, 141.24it/s]\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.55 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/9        0G    0.0214   0.01537         0        23       640: 100% 92/92 [25:02<00:00, 16.34s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:18<00:00,  8.90s/it]\n",
            "                 all        688        749      0.942      0.948      0.963      0.737\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/9        0G   0.02303   0.01527         0        21       640: 100% 92/92 [24:51<00:00, 16.21s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:23<00:00,  9.08s/it]\n",
            "                 all        688        749      0.949      0.939      0.966      0.732\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/9        0G   0.02373   0.01488         0        21       640: 100% 92/92 [25:11<00:00, 16.42s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:22<00:00,  9.04s/it]\n",
            "                 all        688        749      0.946      0.927       0.97      0.699\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/9        0G   0.02466   0.01496         0        17       640: 100% 92/92 [25:19<00:00, 16.52s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:25<00:00,  9.14s/it]\n",
            "                 all        688        749      0.921      0.892      0.949      0.673\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/9        0G   0.02432    0.0153         0        26       640: 100% 92/92 [25:13<00:00, 16.45s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:21<00:00,  9.00s/it]\n",
            "                 all        688        749      0.937      0.928       0.96      0.699\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/9        0G   0.02327   0.01526         0        24       640: 100% 92/92 [25:02<00:00, 16.33s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:23<00:00,  9.09s/it]\n",
            "                 all        688        749      0.939      0.923      0.957      0.688\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/9        0G   0.02204   0.01478         0        17       640: 100% 92/92 [25:14<00:00, 16.46s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:25<00:00,  9.17s/it]\n",
            "                 all        688        749      0.937      0.937      0.962      0.713\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       7/9        0G   0.02107   0.01445         0        27       640: 100% 92/92 [25:29<00:00, 16.63s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:25<00:00,  9.16s/it]\n",
            "                 all        688        749      0.949      0.925      0.963      0.726\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       8/9        0G   0.02005   0.01397         0        29       640: 100% 92/92 [25:13<00:00, 16.45s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:23<00:00,  9.08s/it]\n",
            "                 all        688        749      0.949      0.937      0.971      0.745\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       9/9        0G   0.02126   0.01396         0        25       640: 100% 92/92 [25:03<00:00, 16.34s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:22<00:00,  9.05s/it]\n",
            "                 all        688        749       0.96      0.936      0.969      0.752\n",
            "\n",
            "10 epochs completed in 4.927 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:07<00:00,  8.55s/it]\n",
            "                 all        688        749       0.96      0.936      0.969      0.752\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python val.py --weights /content/yolov5/runs/train/exp/weights/best.pt --data /content/yolov5/data/custom_dataset.yaml --img 640\n",
        "!python detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --img 640 --conf 0.5 --source /content/fypdataset/images/testing\n"
      ],
      "metadata": {
        "id": "1aSwwfxiz71h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/yolov5/runs/train/exp.zip -r /content/yolov5/runs/train/exp"
      ],
      "metadata": {
        "id": "Aev7dl4J23nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/best.pt --img 640 --conf 0.5 --source /content/video1.mp4"
      ],
      "metadata": {
        "id": "zRyWpYD62w-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Training chunk5:"
      ],
      "metadata": {
        "id": "6VNzSr-o0lHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 12 --epochs 10  --data /content/yolov5/data/custom_dataset.yaml --weights /content/best.pt --cache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5YccTi70joi",
        "outputId": "801717a7-313f-45f9-f5b3-a2d1befde19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/best.pt, cfg=, data=/content/yolov5/data/custom_dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=12, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v6.1-307-g92e47b8 Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 18.3MB/s]\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 270 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 349/349 items from /content/best.pt\n",
            "Scaled weight_decay = 0.00046875\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/fypdataset/labels/training5' images and labels...1100 found, 0 missing, 0 empty, 0 corrupt: 100% 1100/1100 [00:00<00:00, 1482.44it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/fypdataset/labels/training5.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB ram): 100% 1100/1100 [00:06<00:00, 175.82it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/fypdataset/labels/validation' images and labels...688 found, 0 missing, 0 empty, 0 corrupt: 100% 688/688 [00:00<00:00, 726.13it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/fypdataset/labels/validation.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.6GB ram): 100% 688/688 [00:04<00:00, 169.09it/s]\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.53 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/9        0G   0.01879   0.01389         0        22       640: 100% 92/92 [24:27<00:00, 15.95s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:05<00:00,  8.48s/it]\n",
            "                 all        688        749      0.943      0.949      0.973       0.76\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/9        0G   0.02081   0.01418         0        20       640: 100% 92/92 [24:39<00:00, 16.08s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:07<00:00,  8.54s/it]\n",
            "                 all        688        749      0.966      0.931      0.971      0.759\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/9        0G   0.02183   0.01375         0        23       640: 100% 92/92 [24:34<00:00, 16.03s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:06<00:00,  8.49s/it]\n",
            "                 all        688        749       0.95      0.937      0.961      0.702\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/9        0G   0.02236   0.01374         0        14       640: 100% 92/92 [24:20<00:00, 15.88s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:06<00:00,  8.49s/it]\n",
            "                 all        688        749      0.949      0.908      0.962      0.712\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/9        0G   0.02187   0.01423         0        20       640: 100% 92/92 [24:18<00:00, 15.86s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:02<00:00,  8.36s/it]\n",
            "                 all        688        749      0.945      0.932      0.962      0.696\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/9        0G   0.02351   0.01406         0        25       640: 100% 92/92 [23:43<00:00, 15.47s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:57<00:00,  8.19s/it]\n",
            "                 all        688        749      0.955      0.938      0.966      0.727\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/9        0G   0.02106   0.01429         0        17       640: 100% 92/92 [24:10<00:00, 15.77s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:03<00:00,  8.39s/it]\n",
            "                 all        688        749      0.946       0.94       0.97      0.717\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       7/9        0G   0.02235   0.01349         0        27       640: 100% 92/92 [24:03<00:00, 15.69s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:01<00:00,  8.32s/it]\n",
            "                 all        688        749      0.965       0.92      0.966      0.732\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       8/9        0G   0.02011   0.01328         0        30       640: 100% 92/92 [23:55<00:00, 15.61s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [04:00<00:00,  8.30s/it]\n",
            "                 all        688        749      0.968      0.944      0.971       0.75\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       9/9        0G   0.01762   0.01306         0        20       640: 100% 92/92 [23:55<00:00, 15.60s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:58<00:00,  8.21s/it]\n",
            "                 all        688        749      0.972      0.941      0.971      0.758\n",
            "\n",
            "10 epochs completed in 4.712 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 29/29 [03:45<00:00,  7.78s/it]\n",
            "                 all        688        749      0.944      0.949      0.974      0.759\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/yolov5/runs/train/exp -r /content/yolov5/runs/train/exp"
      ],
      "metadata": {
        "id": "E-zAmGgGHAKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python val.py --weights /content/yolov5/runs/train/exp/weights/best.pt --data /content/yolov5/data/custom_dataset.yaml --img 640\n",
        "!python detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --img 640 --conf 0.5 --source /content/fypdataset/images/testing\n"
      ],
      "metadata": {
        "id": "GQlwoLSO1oYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --img 640 --conf 0.5 --source /content/video1.mp4\n",
        "!python detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --img 640 --conf 0.5 --source /content/video2.mp4"
      ],
      "metadata": {
        "id": "LbfmyLXCqBzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/yolov5/runs/val/exp -r /content/yolov5/runs/val/exp"
      ],
      "metadata": {
        "id": "LS6i35G_oMQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/yolov5/runs/detect/exp5 -r /content/yolov5/runs/detect/exp5"
      ],
      "metadata": {
        "id": "moOMKjVxvbXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/dogs.zip"
      ],
      "metadata": {
        "id": "nP1utx21xuzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --img 640 --conf 0.9 --source /content/yolov5/dogs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSZztRnLxyOm",
        "outputId": "414c837a-f82a-4a63-d251-ccb088c89469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/yolov5/runs/train/exp/weights/best.pt'], source=/content/yolov5/dogs, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.9, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 ðŸš€ v6.1-307-g92e47b8 Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "image 1/180 /content/yolov5/dogs/5d8fa8bb0c0b0.jpg: 288x640 Done. (0.186s)\n",
            "image 2/180 /content/yolov5/dogs/download (1).jpg: 448x640 Done. (0.292s)\n",
            "image 3/180 /content/yolov5/dogs/download (10).jpg: 352x640 Done. (0.219s)\n",
            "image 4/180 /content/yolov5/dogs/download (11).jpg: 416x640 Done. (0.266s)\n",
            "image 5/180 /content/yolov5/dogs/download (12).jpg: 384x640 Done. (0.239s)\n",
            "image 6/180 /content/yolov5/dogs/download (13).jpg: 384x640 Done. (0.231s)\n",
            "image 7/180 /content/yolov5/dogs/download (14).jpg: 384x640 Done. (0.226s)\n",
            "image 8/180 /content/yolov5/dogs/download (15).jpg: 384x640 Done. (0.231s)\n",
            "image 9/180 /content/yolov5/dogs/download (16).jpg: 448x640 Done. (0.284s)\n",
            "image 10/180 /content/yolov5/dogs/download (17).jpg: 384x640 Done. (0.232s)\n",
            "image 11/180 /content/yolov5/dogs/download (18).jpg: 640x640 Done. (0.406s)\n",
            "image 12/180 /content/yolov5/dogs/download (19).jpg: 480x640 Done. (0.309s)\n",
            "image 13/180 /content/yolov5/dogs/download (2).jpg: 384x640 1 Dog, Done. (0.234s)\n",
            "image 14/180 /content/yolov5/dogs/download (20).jpg: 640x448 Done. (0.280s)\n",
            "image 15/180 /content/yolov5/dogs/download (21).jpg: 480x640 Done. (0.296s)\n",
            "image 16/180 /content/yolov5/dogs/download (22).jpg: 640x480 Done. (0.294s)\n",
            "image 17/180 /content/yolov5/dogs/download (23).jpg: 480x640 Done. (0.286s)\n",
            "image 18/180 /content/yolov5/dogs/download (24).jpg: 480x640 Done. (0.272s)\n",
            "image 19/180 /content/yolov5/dogs/download (25).jpg: 448x640 1 Dog, Done. (0.271s)\n",
            "image 20/180 /content/yolov5/dogs/download (26).jpg: 480x640 Done. (0.286s)\n",
            "image 21/180 /content/yolov5/dogs/download (27).jpg: 640x640 Done. (0.389s)\n",
            "image 22/180 /content/yolov5/dogs/download (28).jpg: 448x640 1 Dog, Done. (0.281s)\n",
            "image 23/180 /content/yolov5/dogs/download (29).jpg: 448x640 Done. (0.271s)\n",
            "image 24/180 /content/yolov5/dogs/download (3).jpg: 352x640 Done. (0.215s)\n",
            "image 25/180 /content/yolov5/dogs/download (30).jpg: 448x640 1 Dog, Done. (0.275s)\n",
            "image 26/180 /content/yolov5/dogs/download (31).jpg: 640x640 Done. (0.396s)\n",
            "image 27/180 /content/yolov5/dogs/download (32).jpg: 384x640 Done. (0.232s)\n",
            "image 28/180 /content/yolov5/dogs/download (33).jpg: 448x640 1 Dog, Done. (0.271s)\n",
            "image 29/180 /content/yolov5/dogs/download (34).jpg: 384x640 Done. (0.229s)\n",
            "image 30/180 /content/yolov5/dogs/download (35).jpg: 448x640 Done. (0.283s)\n",
            "image 31/180 /content/yolov5/dogs/download (4).jpg: 384x640 Done. (0.233s)\n",
            "image 32/180 /content/yolov5/dogs/download (5).jpg: 448x640 Done. (0.261s)\n",
            "image 33/180 /content/yolov5/dogs/download (6).jpg: 384x640 Done. (0.229s)\n",
            "image 34/180 /content/yolov5/dogs/download (7).jpg: 384x640 Done. (0.242s)\n",
            "image 35/180 /content/yolov5/dogs/download (8).jpg: 448x640 Done. (0.270s)\n",
            "image 36/180 /content/yolov5/dogs/download (9).jpg: 352x640 Done. (0.205s)\n",
            "image 37/180 /content/yolov5/dogs/download.jpg: 480x640 Done. (0.280s)\n",
            "image 38/180 /content/yolov5/dogs/images (1).jpg: 480x640 Done. (0.298s)\n",
            "image 39/180 /content/yolov5/dogs/images (10).jpg: 384x640 Done. (0.235s)\n",
            "image 40/180 /content/yolov5/dogs/images (100).jpg: 416x640 Done. (0.256s)\n",
            "image 41/180 /content/yolov5/dogs/images (11).jpg: 352x640 Done. (0.218s)\n",
            "image 42/180 /content/yolov5/dogs/images (12).jpg: 480x640 Done. (0.289s)\n",
            "image 43/180 /content/yolov5/dogs/images (13).jpg: 384x640 Done. (0.230s)\n",
            "image 44/180 /content/yolov5/dogs/images (14).jpg: 416x640 Done. (0.253s)\n",
            "image 45/180 /content/yolov5/dogs/images (15).jpg: 384x640 Done. (0.238s)\n",
            "image 46/180 /content/yolov5/dogs/images (16).jpg: 480x640 Done. (0.295s)\n",
            "image 47/180 /content/yolov5/dogs/images (17).jpg: 416x640 Done. (0.248s)\n",
            "image 48/180 /content/yolov5/dogs/images (18).jpg: 480x640 Done. (0.283s)\n",
            "image 49/180 /content/yolov5/dogs/images (19).jpg: 384x640 Done. (0.242s)\n",
            "image 50/180 /content/yolov5/dogs/images (2).jpg: 480x640 1 Dog, Done. (0.288s)\n",
            "image 51/180 /content/yolov5/dogs/images (20).jpg: 352x640 1 Dog, Done. (0.211s)\n",
            "image 52/180 /content/yolov5/dogs/images (21).jpg: 384x640 2 Dogs, Done. (0.230s)\n",
            "image 53/180 /content/yolov5/dogs/images (22).jpg: 384x640 1 Dog, Done. (0.242s)\n",
            "image 54/180 /content/yolov5/dogs/images (23).jpg: 320x640 2 Dogs, Done. (0.205s)\n",
            "image 55/180 /content/yolov5/dogs/images (24).jpg: 416x640 Done. (0.257s)\n",
            "image 56/180 /content/yolov5/dogs/images (25).jpg: 480x640 Done. (0.294s)\n",
            "image 57/180 /content/yolov5/dogs/images (26).jpg: 480x640 Done. (0.320s)\n",
            "image 58/180 /content/yolov5/dogs/images (27).jpg: 384x640 Done. (0.234s)\n",
            "image 59/180 /content/yolov5/dogs/images (28).jpg: 448x640 Done. (0.273s)\n",
            "image 60/180 /content/yolov5/dogs/images (29).jpg: 384x640 Done. (0.240s)\n",
            "image 61/180 /content/yolov5/dogs/images (3).jpg: 448x640 Done. (0.292s)\n",
            "image 62/180 /content/yolov5/dogs/images (30).jpg: 384x640 Done. (0.238s)\n",
            "image 63/180 /content/yolov5/dogs/images (31).jpg: 384x640 Done. (0.228s)\n",
            "image 64/180 /content/yolov5/dogs/images (32).jpg: 480x640 Done. (0.289s)\n",
            "image 65/180 /content/yolov5/dogs/images (33).jpg: 416x640 Done. (0.267s)\n",
            "image 66/180 /content/yolov5/dogs/images (34).jpg: 384x640 1 Dog, Done. (0.232s)\n",
            "image 67/180 /content/yolov5/dogs/images (35).jpg: 384x640 1 Dog, Done. (0.238s)\n",
            "image 68/180 /content/yolov5/dogs/images (36).jpg: 480x640 Done. (0.291s)\n",
            "image 69/180 /content/yolov5/dogs/images (37).jpg: 448x640 Done. (0.279s)\n",
            "image 70/180 /content/yolov5/dogs/images (38).jpg: 352x640 Done. (0.210s)\n",
            "image 71/180 /content/yolov5/dogs/images (39).jpg: 480x640 Done. (0.278s)\n",
            "image 72/180 /content/yolov5/dogs/images (4).jpg: 352x640 1 Dog, Done. (0.210s)\n",
            "image 73/180 /content/yolov5/dogs/images (40).jpg: 288x640 Done. (0.188s)\n",
            "image 74/180 /content/yolov5/dogs/images (41).jpg: 384x640 Done. (0.239s)\n",
            "image 75/180 /content/yolov5/dogs/images (42).jpg: 448x640 Done. (0.279s)\n",
            "image 76/180 /content/yolov5/dogs/images (43).jpg: 416x640 Done. (0.251s)\n",
            "image 77/180 /content/yolov5/dogs/images (44).jpg: 384x640 Done. (0.245s)\n",
            "image 78/180 /content/yolov5/dogs/images (45).jpg: 480x640 1 Dog, Done. (0.284s)\n",
            "image 79/180 /content/yolov5/dogs/images (46).jpg: 352x640 Done. (0.209s)\n",
            "image 80/180 /content/yolov5/dogs/images (47).jpg: 480x640 Done. (0.290s)\n",
            "image 81/180 /content/yolov5/dogs/images (48).jpg: 640x640 Done. (0.395s)\n",
            "image 82/180 /content/yolov5/dogs/images (49).jpg: 448x640 Done. (0.269s)\n",
            "image 83/180 /content/yolov5/dogs/images (5).jpg: 352x640 Done. (0.211s)\n",
            "image 84/180 /content/yolov5/dogs/images (50).jpg: 384x640 Done. (0.235s)\n",
            "image 85/180 /content/yolov5/dogs/images (51).jpg: 384x640 Done. (0.236s)\n",
            "image 86/180 /content/yolov5/dogs/images (52).jpg: 480x640 Done. (0.295s)\n",
            "image 87/180 /content/yolov5/dogs/images (53).jpg: 320x640 1 Dog, Done. (0.199s)\n",
            "image 88/180 /content/yolov5/dogs/images (54).jpg: 384x640 Done. (0.235s)\n",
            "image 89/180 /content/yolov5/dogs/images (55).jpg: 352x640 1 Dog, Done. (0.230s)\n",
            "image 90/180 /content/yolov5/dogs/images (56).jpg: 320x640 Done. (0.204s)\n",
            "image 91/180 /content/yolov5/dogs/images (57).jpg: 384x640 1 Dog, Done. (0.243s)\n",
            "image 92/180 /content/yolov5/dogs/images (58).jpg: 480x640 1 Dog, Done. (0.298s)\n",
            "image 93/180 /content/yolov5/dogs/images (59).jpg: 384x640 1 Dog, Done. (0.245s)\n",
            "image 94/180 /content/yolov5/dogs/images (6).jpg: 448x640 Done. (0.274s)\n",
            "image 95/180 /content/yolov5/dogs/images (60).jpg: 448x640 Done. (0.273s)\n",
            "image 96/180 /content/yolov5/dogs/images (61).jpg: 352x640 Done. (0.221s)\n",
            "image 97/180 /content/yolov5/dogs/images (62).jpg: 480x640 Done. (0.310s)\n",
            "image 98/180 /content/yolov5/dogs/images (63).jpg: 416x640 Done. (0.270s)\n",
            "image 99/180 /content/yolov5/dogs/images (64).jpg: 416x640 Done. (0.271s)\n",
            "image 100/180 /content/yolov5/dogs/images (65).jpg: 448x640 1 Dog, Done. (0.291s)\n",
            "image 101/180 /content/yolov5/dogs/images (66).jpg: 320x640 Done. (0.202s)\n",
            "image 102/180 /content/yolov5/dogs/images (67).jpg: 384x640 1 Dog, Done. (0.248s)\n",
            "image 103/180 /content/yolov5/dogs/images (68).jpg: 384x640 1 Dog, Done. (0.240s)\n",
            "image 104/180 /content/yolov5/dogs/images (69).jpg: 384x640 1 Dog, Done. (0.242s)\n",
            "image 105/180 /content/yolov5/dogs/images (7).jpg: 384x640 Done. (0.236s)\n",
            "image 106/180 /content/yolov5/dogs/images (70).jpg: 480x640 Done. (0.296s)\n",
            "image 107/180 /content/yolov5/dogs/images (71).jpg: 416x640 1 Dog, Done. (0.254s)\n",
            "image 108/180 /content/yolov5/dogs/images (72).jpg: 384x640 1 Dog, Done. (0.242s)\n",
            "image 109/180 /content/yolov5/dogs/images (73).jpg: 384x640 Done. (0.236s)\n",
            "image 110/180 /content/yolov5/dogs/images (74).jpg: 448x640 Done. (0.273s)\n",
            "image 111/180 /content/yolov5/dogs/images (75).jpg: 480x640 Done. (0.301s)\n",
            "image 112/180 /content/yolov5/dogs/images (76).jpg: 384x640 Done. (0.259s)\n",
            "image 113/180 /content/yolov5/dogs/images (77).jpg: 416x640 2 Dogs, Done. (0.264s)\n",
            "image 114/180 /content/yolov5/dogs/images (78).jpg: 448x640 Done. (0.286s)\n",
            "image 115/180 /content/yolov5/dogs/images (79).jpg: 384x640 Done. (0.241s)\n",
            "image 116/180 /content/yolov5/dogs/images (8).jpg: 480x640 Done. (0.304s)\n",
            "image 117/180 /content/yolov5/dogs/images (80).jpg: 384x640 Done. (0.237s)\n",
            "image 118/180 /content/yolov5/dogs/images (81).jpg: 448x640 Done. (0.279s)\n",
            "image 119/180 /content/yolov5/dogs/images (82).jpg: 480x640 Done. (0.301s)\n",
            "image 120/180 /content/yolov5/dogs/images (83).jpg: 480x640 Done. (0.280s)\n",
            "image 121/180 /content/yolov5/dogs/images (84).jpg: 384x640 Done. (0.231s)\n",
            "image 122/180 /content/yolov5/dogs/images (85).jpg: 384x640 Done. (0.239s)\n",
            "image 123/180 /content/yolov5/dogs/images (86).jpg: 384x640 Done. (0.237s)\n",
            "image 124/180 /content/yolov5/dogs/images (87).jpg: 640x640 Done. (0.381s)\n",
            "image 125/180 /content/yolov5/dogs/images (88).jpg: 480x640 Done. (0.284s)\n",
            "image 126/180 /content/yolov5/dogs/images (89).jpg: 384x640 Done. (0.229s)\n",
            "image 127/180 /content/yolov5/dogs/images (9).jpg: 352x640 Done. (0.225s)\n",
            "image 128/180 /content/yolov5/dogs/images (90).jpg: 448x640 Done. (0.266s)\n",
            "image 129/180 /content/yolov5/dogs/images (91).jpg: 448x640 Done. (0.272s)\n",
            "image 130/180 /content/yolov5/dogs/images (92).jpg: 480x640 Done. (0.292s)\n",
            "image 131/180 /content/yolov5/dogs/images (93).jpg: 544x640 Done. (0.340s)\n",
            "image 132/180 /content/yolov5/dogs/images (94).jpg: 448x640 Done. (0.269s)\n",
            "image 133/180 /content/yolov5/dogs/images (95).jpg: 384x640 Done. (0.227s)\n",
            "image 134/180 /content/yolov5/dogs/images (96).jpg: 640x640 Done. (0.398s)\n",
            "image 135/180 /content/yolov5/dogs/images (97).jpg: 384x640 Done. (0.238s)\n",
            "image 136/180 /content/yolov5/dogs/images (98).jpg: 448x640 1 Dog, Done. (0.272s)\n",
            "image 137/180 /content/yolov5/dogs/images (99).jpg: 448x640 Done. (0.260s)\n",
            "image 138/180 /content/yolov5/dogs/images - 2022-04-24T173026.205.jpg: 480x640 Done. (0.288s)\n",
            "image 139/180 /content/yolov5/dogs/images - 2022-04-24T173039.522.jpg: 640x640 Done. (0.392s)\n",
            "image 140/180 /content/yolov5/dogs/images - 2022-04-24T173043.365.jpg: 640x448 1 Dog, Done. (0.273s)\n",
            "image 141/180 /content/yolov5/dogs/images - 2022-04-24T173046.987.jpg: 640x384 Done. (0.242s)\n",
            "image 142/180 /content/yolov5/dogs/images - 2022-04-24T173051.067.jpg: 480x640 1 Dog, Done. (0.302s)\n",
            "image 143/180 /content/yolov5/dogs/images - 2022-04-24T173055.552.jpg: 640x640 Done. (0.396s)\n",
            "image 144/180 /content/yolov5/dogs/images - 2022-04-24T173101.027.jpg: 480x640 Done. (0.307s)\n",
            "image 145/180 /content/yolov5/dogs/images - 2022-04-24T173105.378.jpg: 576x640 Done. (0.353s)\n",
            "image 146/180 /content/yolov5/dogs/images - 2022-04-24T173111.254.jpg: 384x640 Done. (0.229s)\n",
            "image 147/180 /content/yolov5/dogs/images - 2022-04-24T173114.353.jpg: 320x640 Done. (0.192s)\n",
            "image 148/180 /content/yolov5/dogs/images - 2022-04-24T173120.592.jpg: 448x640 Done. (0.278s)\n",
            "image 149/180 /content/yolov5/dogs/images - 2022-04-24T173136.045.jpg: 480x640 1 Dog, Done. (0.287s)\n",
            "image 150/180 /content/yolov5/dogs/images - 2022-04-24T173146.789.jpg: 544x640 Done. (0.323s)\n",
            "image 151/180 /content/yolov5/dogs/images - 2022-04-24T173150.635.jpg: 320x640 1 Dog, Done. (0.206s)\n",
            "image 152/180 /content/yolov5/dogs/images - 2022-04-24T173157.348.jpg: 480x640 1 Dog, Done. (0.303s)\n",
            "image 153/180 /content/yolov5/dogs/images - 2022-04-24T173207.289.jpg: 352x640 Done. (0.213s)\n",
            "image 154/180 /content/yolov5/dogs/images - 2022-04-24T173212.852.jpg: 448x640 Done. (0.274s)\n",
            "image 155/180 /content/yolov5/dogs/images - 2022-04-24T173348.862.jpg: 608x640 Done. (0.400s)\n",
            "image 156/180 /content/yolov5/dogs/images - 2022-04-24T173404.112.jpg: 384x640 Done. (0.245s)\n",
            "image 157/180 /content/yolov5/dogs/images - 2022-04-24T173506.535.jpg: 288x640 Done. (0.191s)\n",
            "image 158/180 /content/yolov5/dogs/images - 2022-04-24T173514.794.jpg: 384x640 Done. (0.230s)\n",
            "image 159/180 /content/yolov5/dogs/images - 2022-04-24T173517.965.jpg: 384x640 Done. (0.246s)\n",
            "image 160/180 /content/yolov5/dogs/images - 2022-04-24T173524.145.jpg: 320x640 1 Dog, Done. (0.198s)\n",
            "image 161/180 /content/yolov5/dogs/images - 2022-04-24T173529.322.jpg: 384x640 Done. (0.241s)\n",
            "image 162/180 /content/yolov5/dogs/images - 2022-04-24T173537.212.jpg: 384x640 Done. (0.243s)\n",
            "image 163/180 /content/yolov5/dogs/images - 2022-04-24T173541.020.jpg: 384x640 Done. (0.241s)\n",
            "image 164/180 /content/yolov5/dogs/images - 2022-04-24T173549.872.jpg: 448x640 Done. (0.273s)\n",
            "image 165/180 /content/yolov5/dogs/images - 2022-04-24T173601.634.jpg: 384x640 1 Dog, Done. (0.234s)\n",
            "image 166/180 /content/yolov5/dogs/images - 2022-04-24T173610.609.jpg: 480x640 Done. (0.287s)\n",
            "image 167/180 /content/yolov5/dogs/images - 2022-04-24T173613.818.jpg: 448x640 Done. (0.281s)\n",
            "image 168/180 /content/yolov5/dogs/images - 2022-04-24T173624.282.jpg: 384x640 Done. (0.236s)\n",
            "image 169/180 /content/yolov5/dogs/images - 2022-04-24T173628.619.jpg: 480x640 Done. (0.303s)\n",
            "image 170/180 /content/yolov5/dogs/images - 2022-04-24T173637.499.jpg: 416x640 1 Dog, Done. (0.261s)\n",
            "image 171/180 /content/yolov5/dogs/images - 2022-04-24T173642.392.jpg: 384x640 Done. (0.247s)\n",
            "image 172/180 /content/yolov5/dogs/images - 2022-04-24T173646.052.jpg: 384x640 1 Dog, Done. (0.237s)\n",
            "image 173/180 /content/yolov5/dogs/images - 2022-04-24T173651.596.jpg: 480x640 Done. (0.292s)\n",
            "image 174/180 /content/yolov5/dogs/images - 2022-04-24T173655.953.jpg: 640x576 1 Dog, Done. (0.367s)\n",
            "image 175/180 /content/yolov5/dogs/images - 2022-04-24T173659.642.jpg: 384x640 1 Dog, Done. (0.252s)\n",
            "image 176/180 /content/yolov5/dogs/images - 2022-04-24T173706.435.jpg: 384x640 Done. (0.237s)\n",
            "image 177/180 /content/yolov5/dogs/images - 2022-04-24T173718.721.jpg: 448x640 Done. (0.272s)\n",
            "image 178/180 /content/yolov5/dogs/images - 2022-04-24T173736.147.jpg: 416x640 1 Dog, Done. (0.300s)\n",
            "image 179/180 /content/yolov5/dogs/images - 2022-04-24T173748.352.jpg: 384x640 Done. (0.239s)\n",
            "image 180/180 /content/yolov5/dogs/images.jpg: 384x640 Done. (0.243s)\n",
            "Speed: 1.2ms pre-process, 264.2ms inference, 0.3ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp8\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/yolov5/runs/detect/exp8 -r /content/yolov5/runs/detect/exp8"
      ],
      "metadata": {
        "id": "rf7IZlfwyYeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/best.pt --img 640 --conf 0.9 --source /content/yolov5/dogs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cenpAp90yzdH",
        "outputId": "f5ff0a36-2767-4a5f-a0ba-be002194b8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/best.pt'], source=/content/yolov5/dogs, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.9, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 ðŸš€ v6.1-307-g92e47b8 Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "image 1/180 /content/yolov5/dogs/5d8fa8bb0c0b0.jpg: 288x640 Done. (0.199s)\n",
            "image 2/180 /content/yolov5/dogs/download (1).jpg: 448x640 Done. (0.287s)\n",
            "image 3/180 /content/yolov5/dogs/download (10).jpg: 352x640 Done. (0.236s)\n",
            "image 4/180 /content/yolov5/dogs/download (11).jpg: 416x640 Done. (0.267s)\n",
            "image 5/180 /content/yolov5/dogs/download (12).jpg: 384x640 Done. (0.252s)\n",
            "image 6/180 /content/yolov5/dogs/download (13).jpg: 384x640 Done. (0.233s)\n",
            "image 7/180 /content/yolov5/dogs/download (14).jpg: 384x640 Done. (0.258s)\n",
            "image 8/180 /content/yolov5/dogs/download (15).jpg: 384x640 Done. (0.237s)\n",
            "image 9/180 /content/yolov5/dogs/download (16).jpg: 448x640 Done. (0.277s)\n",
            "image 10/180 /content/yolov5/dogs/download (17).jpg: 384x640 Done. (0.246s)\n",
            "image 11/180 /content/yolov5/dogs/download (18).jpg: 640x640 Done. (0.414s)\n",
            "image 12/180 /content/yolov5/dogs/download (19).jpg: 480x640 Done. (0.301s)\n",
            "image 13/180 /content/yolov5/dogs/download (2).jpg: 384x640 1 Dog, Done. (0.226s)\n",
            "image 14/180 /content/yolov5/dogs/download (20).jpg: 640x448 Done. (0.283s)\n",
            "image 15/180 /content/yolov5/dogs/download (21).jpg: 480x640 Done. (0.284s)\n",
            "image 16/180 /content/yolov5/dogs/download (22).jpg: 640x480 Done. (0.295s)\n",
            "image 17/180 /content/yolov5/dogs/download (23).jpg: 480x640 Done. (0.293s)\n",
            "image 18/180 /content/yolov5/dogs/download (24).jpg: 480x640 Done. (0.297s)\n",
            "image 19/180 /content/yolov5/dogs/download (25).jpg: 448x640 Done. (0.287s)\n",
            "image 20/180 /content/yolov5/dogs/download (26).jpg: 480x640 Done. (0.294s)\n",
            "image 21/180 /content/yolov5/dogs/download (27).jpg: 640x640 Done. (0.398s)\n",
            "image 22/180 /content/yolov5/dogs/download (28).jpg: 448x640 1 Dog, Done. (0.266s)\n",
            "image 23/180 /content/yolov5/dogs/download (29).jpg: 448x640 Done. (0.283s)\n",
            "image 24/180 /content/yolov5/dogs/download (3).jpg: 352x640 Done. (0.209s)\n",
            "image 25/180 /content/yolov5/dogs/download (30).jpg: 448x640 1 Dog, Done. (0.282s)\n",
            "image 26/180 /content/yolov5/dogs/download (31).jpg: 640x640 Done. (0.384s)\n",
            "image 27/180 /content/yolov5/dogs/download (32).jpg: 384x640 Done. (0.227s)\n",
            "image 28/180 /content/yolov5/dogs/download (33).jpg: 448x640 Done. (0.284s)\n",
            "image 29/180 /content/yolov5/dogs/download (34).jpg: 384x640 Done. (0.240s)\n",
            "image 30/180 /content/yolov5/dogs/download (35).jpg: 448x640 Done. (0.283s)\n",
            "image 31/180 /content/yolov5/dogs/download (4).jpg: 384x640 Done. (0.242s)\n",
            "image 32/180 /content/yolov5/dogs/download (5).jpg: 448x640 Done. (0.283s)\n",
            "image 33/180 /content/yolov5/dogs/download (6).jpg: 384x640 Done. (0.244s)\n",
            "image 34/180 /content/yolov5/dogs/download (7).jpg: 384x640 Done. (0.234s)\n",
            "image 35/180 /content/yolov5/dogs/download (8).jpg: 448x640 Done. (0.266s)\n",
            "image 36/180 /content/yolov5/dogs/download (9).jpg: 352x640 Done. (0.223s)\n",
            "image 37/180 /content/yolov5/dogs/download.jpg: 480x640 Done. (0.296s)\n",
            "image 38/180 /content/yolov5/dogs/images (1).jpg: 480x640 Done. (0.289s)\n",
            "image 39/180 /content/yolov5/dogs/images (10).jpg: 384x640 Done. (0.237s)\n",
            "image 40/180 /content/yolov5/dogs/images (100).jpg: 416x640 Done. (0.255s)\n",
            "image 41/180 /content/yolov5/dogs/images (11).jpg: 352x640 Done. (0.213s)\n",
            "image 42/180 /content/yolov5/dogs/images (12).jpg: 480x640 Done. (0.287s)\n",
            "image 43/180 /content/yolov5/dogs/images (13).jpg: 384x640 Done. (0.239s)\n",
            "image 44/180 /content/yolov5/dogs/images (14).jpg: 416x640 Done. (0.256s)\n",
            "image 45/180 /content/yolov5/dogs/images (15).jpg: 384x640 Done. (0.236s)\n",
            "image 46/180 /content/yolov5/dogs/images (16).jpg: 480x640 Done. (0.291s)\n",
            "image 47/180 /content/yolov5/dogs/images (17).jpg: 416x640 Done. (0.283s)\n",
            "image 48/180 /content/yolov5/dogs/images (18).jpg: 480x640 Done. (0.299s)\n",
            "image 49/180 /content/yolov5/dogs/images (19).jpg: 384x640 Done. (0.235s)\n",
            "image 50/180 /content/yolov5/dogs/images (2).jpg: 480x640 Done. (0.292s)\n",
            "image 51/180 /content/yolov5/dogs/images (20).jpg: 352x640 1 Dog, Done. (0.222s)\n",
            "image 52/180 /content/yolov5/dogs/images (21).jpg: 384x640 2 Dogs, Done. (0.236s)\n",
            "image 53/180 /content/yolov5/dogs/images (22).jpg: 384x640 1 Dog, Done. (0.232s)\n",
            "image 54/180 /content/yolov5/dogs/images (23).jpg: 320x640 1 Dog, Done. (0.207s)\n",
            "image 55/180 /content/yolov5/dogs/images (24).jpg: 416x640 Done. (0.268s)\n",
            "image 56/180 /content/yolov5/dogs/images (25).jpg: 480x640 Done. (0.295s)\n",
            "image 57/180 /content/yolov5/dogs/images (26).jpg: 480x640 Done. (0.290s)\n",
            "image 58/180 /content/yolov5/dogs/images (27).jpg: 384x640 Done. (0.231s)\n",
            "image 59/180 /content/yolov5/dogs/images (28).jpg: 448x640 Done. (0.285s)\n",
            "image 60/180 /content/yolov5/dogs/images (29).jpg: 384x640 Done. (0.233s)\n",
            "image 61/180 /content/yolov5/dogs/images (3).jpg: 448x640 Done. (0.264s)\n",
            "image 62/180 /content/yolov5/dogs/images (30).jpg: 384x640 Done. (0.246s)\n",
            "image 63/180 /content/yolov5/dogs/images (31).jpg: 384x640 1 Dog, Done. (0.255s)\n",
            "image 64/180 /content/yolov5/dogs/images (32).jpg: 480x640 Done. (0.295s)\n",
            "image 65/180 /content/yolov5/dogs/images (33).jpg: 416x640 Done. (0.250s)\n",
            "image 66/180 /content/yolov5/dogs/images (34).jpg: 384x640 1 Dog, Done. (0.239s)\n",
            "image 67/180 /content/yolov5/dogs/images (35).jpg: 384x640 1 Dog, Done. (0.251s)\n",
            "image 68/180 /content/yolov5/dogs/images (36).jpg: 480x640 Done. (0.297s)\n",
            "image 69/180 /content/yolov5/dogs/images (37).jpg: 448x640 Done. (0.265s)\n",
            "image 70/180 /content/yolov5/dogs/images (38).jpg: 352x640 Done. (0.220s)\n",
            "image 71/180 /content/yolov5/dogs/images (39).jpg: 480x640 Done. (0.305s)\n",
            "image 72/180 /content/yolov5/dogs/images (4).jpg: 352x640 1 Dog, Done. (0.212s)\n",
            "image 73/180 /content/yolov5/dogs/images (40).jpg: 288x640 Done. (0.183s)\n",
            "image 74/180 /content/yolov5/dogs/images (41).jpg: 384x640 Done. (0.237s)\n",
            "image 75/180 /content/yolov5/dogs/images (42).jpg: 448x640 Done. (0.292s)\n",
            "image 76/180 /content/yolov5/dogs/images (43).jpg: 416x640 Done. (0.263s)\n",
            "image 77/180 /content/yolov5/dogs/images (44).jpg: 384x640 Done. (0.242s)\n",
            "image 78/180 /content/yolov5/dogs/images (45).jpg: 480x640 1 Dog, Done. (0.307s)\n",
            "image 79/180 /content/yolov5/dogs/images (46).jpg: 352x640 Done. (0.219s)\n",
            "image 80/180 /content/yolov5/dogs/images (47).jpg: 480x640 Done. (0.292s)\n",
            "image 81/180 /content/yolov5/dogs/images (48).jpg: 640x640 Done. (0.406s)\n",
            "image 82/180 /content/yolov5/dogs/images (49).jpg: 448x640 Done. (0.281s)\n",
            "image 83/180 /content/yolov5/dogs/images (5).jpg: 352x640 Done. (0.203s)\n",
            "image 84/180 /content/yolov5/dogs/images (50).jpg: 384x640 Done. (0.238s)\n",
            "image 85/180 /content/yolov5/dogs/images (51).jpg: 384x640 Done. (0.235s)\n",
            "image 86/180 /content/yolov5/dogs/images (52).jpg: 480x640 Done. (0.300s)\n",
            "image 87/180 /content/yolov5/dogs/images (53).jpg: 320x640 Done. (0.209s)\n",
            "image 88/180 /content/yolov5/dogs/images (54).jpg: 384x640 Done. (0.244s)\n",
            "image 89/180 /content/yolov5/dogs/images (55).jpg: 352x640 1 Dog, Done. (0.221s)\n",
            "image 90/180 /content/yolov5/dogs/images (56).jpg: 320x640 Done. (0.218s)\n",
            "image 91/180 /content/yolov5/dogs/images (57).jpg: 384x640 1 Dog, Done. (0.247s)\n",
            "image 92/180 /content/yolov5/dogs/images (58).jpg: 480x640 Done. (0.295s)\n",
            "image 93/180 /content/yolov5/dogs/images (59).jpg: 384x640 1 Dog, Done. (0.244s)\n",
            "image 94/180 /content/yolov5/dogs/images (6).jpg: 448x640 Done. (0.294s)\n",
            "image 95/180 /content/yolov5/dogs/images (60).jpg: 448x640 1 Dog, Done. (0.280s)\n",
            "image 96/180 /content/yolov5/dogs/images (61).jpg: 352x640 Done. (0.224s)\n",
            "image 97/180 /content/yolov5/dogs/images (62).jpg: 480x640 Done. (0.297s)\n",
            "image 98/180 /content/yolov5/dogs/images (63).jpg: 416x640 Done. (0.265s)\n",
            "image 99/180 /content/yolov5/dogs/images (64).jpg: 416x640 Done. (0.251s)\n",
            "image 100/180 /content/yolov5/dogs/images (65).jpg: 448x640 Done. (0.271s)\n",
            "image 101/180 /content/yolov5/dogs/images (66).jpg: 320x640 Done. (0.190s)\n",
            "image 102/180 /content/yolov5/dogs/images (67).jpg: 384x640 Done. (0.244s)\n",
            "image 103/180 /content/yolov5/dogs/images (68).jpg: 384x640 Done. (0.235s)\n",
            "image 104/180 /content/yolov5/dogs/images (69).jpg: 384x640 Done. (0.239s)\n",
            "image 105/180 /content/yolov5/dogs/images (7).jpg: 384x640 Done. (0.243s)\n",
            "image 106/180 /content/yolov5/dogs/images (70).jpg: 480x640 Done. (0.306s)\n",
            "image 107/180 /content/yolov5/dogs/images (71).jpg: 416x640 1 Dog, Done. (0.250s)\n",
            "image 108/180 /content/yolov5/dogs/images (72).jpg: 384x640 1 Dog, Done. (0.248s)\n",
            "image 109/180 /content/yolov5/dogs/images (73).jpg: 384x640 Done. (0.241s)\n",
            "image 110/180 /content/yolov5/dogs/images (74).jpg: 448x640 Done. (0.278s)\n",
            "image 111/180 /content/yolov5/dogs/images (75).jpg: 480x640 Done. (0.302s)\n",
            "image 112/180 /content/yolov5/dogs/images (76).jpg: 384x640 Done. (0.244s)\n",
            "image 113/180 /content/yolov5/dogs/images (77).jpg: 416x640 2 Dogs, Done. (0.269s)\n",
            "image 114/180 /content/yolov5/dogs/images (78).jpg: 448x640 1 Dog, Done. (0.261s)\n",
            "image 115/180 /content/yolov5/dogs/images (79).jpg: 384x640 Done. (0.229s)\n",
            "image 116/180 /content/yolov5/dogs/images (8).jpg: 480x640 Done. (0.288s)\n",
            "image 117/180 /content/yolov5/dogs/images (80).jpg: 384x640 1 Dog, Done. (0.237s)\n",
            "image 118/180 /content/yolov5/dogs/images (81).jpg: 448x640 Done. (0.275s)\n",
            "image 119/180 /content/yolov5/dogs/images (82).jpg: 480x640 Done. (0.308s)\n",
            "image 120/180 /content/yolov5/dogs/images (83).jpg: 480x640 Done. (0.303s)\n",
            "image 121/180 /content/yolov5/dogs/images (84).jpg: 384x640 Done. (0.254s)\n",
            "image 122/180 /content/yolov5/dogs/images (85).jpg: 384x640 Done. (0.237s)\n",
            "image 123/180 /content/yolov5/dogs/images (86).jpg: 384x640 Done. (0.231s)\n",
            "image 124/180 /content/yolov5/dogs/images (87).jpg: 640x640 Done. (0.396s)\n",
            "image 125/180 /content/yolov5/dogs/images (88).jpg: 480x640 Done. (0.291s)\n",
            "image 126/180 /content/yolov5/dogs/images (89).jpg: 384x640 Done. (0.258s)\n",
            "image 127/180 /content/yolov5/dogs/images (9).jpg: 352x640 1 Dog, Done. (0.219s)\n",
            "image 128/180 /content/yolov5/dogs/images (90).jpg: 448x640 Done. (0.278s)\n",
            "image 129/180 /content/yolov5/dogs/images (91).jpg: 448x640 Done. (0.275s)\n",
            "image 130/180 /content/yolov5/dogs/images (92).jpg: 480x640 Done. (0.289s)\n",
            "image 131/180 /content/yolov5/dogs/images (93).jpg: 544x640 Done. (0.337s)\n",
            "image 132/180 /content/yolov5/dogs/images (94).jpg: 448x640 Done. (0.269s)\n",
            "image 133/180 /content/yolov5/dogs/images (95).jpg: 384x640 Done. (0.234s)\n",
            "image 134/180 /content/yolov5/dogs/images (96).jpg: 640x640 Done. (0.381s)\n",
            "image 135/180 /content/yolov5/dogs/images (97).jpg: 384x640 Done. (0.239s)\n",
            "image 136/180 /content/yolov5/dogs/images (98).jpg: 448x640 1 Dog, Done. (0.268s)\n",
            "image 137/180 /content/yolov5/dogs/images (99).jpg: 448x640 Done. (0.273s)\n",
            "image 138/180 /content/yolov5/dogs/images - 2022-04-24T173026.205.jpg: 480x640 Done. (0.296s)\n",
            "image 139/180 /content/yolov5/dogs/images - 2022-04-24T173039.522.jpg: 640x640 Done. (0.392s)\n",
            "image 140/180 /content/yolov5/dogs/images - 2022-04-24T173043.365.jpg: 640x448 1 Dog, Done. (0.280s)\n",
            "image 141/180 /content/yolov5/dogs/images - 2022-04-24T173046.987.jpg: 640x384 Done. (0.235s)\n",
            "image 142/180 /content/yolov5/dogs/images - 2022-04-24T173051.067.jpg: 480x640 2 Dogs, Done. (0.300s)\n",
            "image 143/180 /content/yolov5/dogs/images - 2022-04-24T173055.552.jpg: 640x640 Done. (0.389s)\n",
            "image 144/180 /content/yolov5/dogs/images - 2022-04-24T173101.027.jpg: 480x640 Done. (0.294s)\n",
            "image 145/180 /content/yolov5/dogs/images - 2022-04-24T173105.378.jpg: 576x640 Done. (0.369s)\n",
            "image 146/180 /content/yolov5/dogs/images - 2022-04-24T173111.254.jpg: 384x640 Done. (0.235s)\n",
            "image 147/180 /content/yolov5/dogs/images - 2022-04-24T173114.353.jpg: 320x640 Done. (0.192s)\n",
            "image 148/180 /content/yolov5/dogs/images - 2022-04-24T173120.592.jpg: 448x640 Done. (0.273s)\n",
            "image 149/180 /content/yolov5/dogs/images - 2022-04-24T173136.045.jpg: 480x640 Done. (0.302s)\n",
            "image 150/180 /content/yolov5/dogs/images - 2022-04-24T173146.789.jpg: 544x640 Done. (0.336s)\n",
            "image 151/180 /content/yolov5/dogs/images - 2022-04-24T173150.635.jpg: 320x640 2 Dogs, Done. (0.201s)\n",
            "image 152/180 /content/yolov5/dogs/images - 2022-04-24T173157.348.jpg: 480x640 Done. (0.297s)\n",
            "image 153/180 /content/yolov5/dogs/images - 2022-04-24T173207.289.jpg: 352x640 Done. (0.226s)\n",
            "image 154/180 /content/yolov5/dogs/images - 2022-04-24T173212.852.jpg: 448x640 Done. (0.278s)\n",
            "image 155/180 /content/yolov5/dogs/images - 2022-04-24T173348.862.jpg: 608x640 Done. (0.380s)\n",
            "image 156/180 /content/yolov5/dogs/images - 2022-04-24T173404.112.jpg: 384x640 Done. (0.247s)\n",
            "image 157/180 /content/yolov5/dogs/images - 2022-04-24T173506.535.jpg: 288x640 Done. (0.190s)\n",
            "image 158/180 /content/yolov5/dogs/images - 2022-04-24T173514.794.jpg: 384x640 Done. (0.249s)\n",
            "image 159/180 /content/yolov5/dogs/images - 2022-04-24T173517.965.jpg: 384x640 Done. (0.243s)\n",
            "image 160/180 /content/yolov5/dogs/images - 2022-04-24T173524.145.jpg: 320x640 1 Dog, Done. (0.209s)\n",
            "image 161/180 /content/yolov5/dogs/images - 2022-04-24T173529.322.jpg: 384x640 Done. (0.248s)\n",
            "image 162/180 /content/yolov5/dogs/images - 2022-04-24T173537.212.jpg: 384x640 Done. (0.240s)\n",
            "image 163/180 /content/yolov5/dogs/images - 2022-04-24T173541.020.jpg: 384x640 Done. (0.255s)\n",
            "image 164/180 /content/yolov5/dogs/images - 2022-04-24T173549.872.jpg: 448x640 Done. (0.282s)\n",
            "image 165/180 /content/yolov5/dogs/images - 2022-04-24T173601.634.jpg: 384x640 Done. (0.234s)\n",
            "image 166/180 /content/yolov5/dogs/images - 2022-04-24T173610.609.jpg: 480x640 Done. (0.297s)\n",
            "image 167/180 /content/yolov5/dogs/images - 2022-04-24T173613.818.jpg: 448x640 Done. (0.279s)\n",
            "image 168/180 /content/yolov5/dogs/images - 2022-04-24T173624.282.jpg: 384x640 Done. (0.247s)\n",
            "image 169/180 /content/yolov5/dogs/images - 2022-04-24T173628.619.jpg: 480x640 Done. (0.283s)\n",
            "image 170/180 /content/yolov5/dogs/images - 2022-04-24T173637.499.jpg: 416x640 Done. (0.246s)\n",
            "image 171/180 /content/yolov5/dogs/images - 2022-04-24T173642.392.jpg: 384x640 Done. (0.228s)\n",
            "image 172/180 /content/yolov5/dogs/images - 2022-04-24T173646.052.jpg: 384x640 1 Dog, Done. (0.243s)\n",
            "image 173/180 /content/yolov5/dogs/images - 2022-04-24T173651.596.jpg: 480x640 Done. (0.278s)\n",
            "image 174/180 /content/yolov5/dogs/images - 2022-04-24T173655.953.jpg: 640x576 Done. (0.339s)\n",
            "image 175/180 /content/yolov5/dogs/images - 2022-04-24T173659.642.jpg: 384x640 1 Dog, Done. (0.231s)\n",
            "image 176/180 /content/yolov5/dogs/images - 2022-04-24T173706.435.jpg: 384x640 Done. (0.226s)\n",
            "image 177/180 /content/yolov5/dogs/images - 2022-04-24T173718.721.jpg: 448x640 Done. (0.265s)\n",
            "image 178/180 /content/yolov5/dogs/images - 2022-04-24T173736.147.jpg: 416x640 1 Dog, Done. (0.251s)\n",
            "image 179/180 /content/yolov5/dogs/images - 2022-04-24T173748.352.jpg: 384x640 Done. (0.244s)\n",
            "image 180/180 /content/yolov5/dogs/images.jpg: 384x640 Done. (0.238s)\n",
            "Speed: 1.2ms pre-process, 265.9ms inference, 0.4ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp9\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/yolov5/runs/detect/exp9 -r /content/yolov5/runs/detect/exp9"
      ],
      "metadata": {
        "id": "Kt0Zf_tKz0K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# until chunk 4 was trained:\n",
        "\n",
        "!python detect.py --weights /content/best.pt --img 640 --conf 0.5 --source /content/yolov5/dogs"
      ],
      "metadata": {
        "id": "J8gpJmlv0RPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#until chunk 5 was trained:\n",
        "\n",
        "!python detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --img 640 --conf 0.5 --source /content/yolov5/dogs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaG5oyjl0RD0",
        "outputId": "7ee0937a-95b9-46d2-86ed-0066d7a6d9b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/yolov5/runs/train/exp/weights/best.pt'], source=/content/yolov5/dogs, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 ðŸš€ v6.1-307-g92e47b8 Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "image 1/180 /content/yolov5/dogs/5d8fa8bb0c0b0.jpg: 288x640 5 Dogs, Done. (0.209s)\n",
            "image 2/180 /content/yolov5/dogs/download (1).jpg: 448x640 2 Dogs, Done. (0.297s)\n",
            "image 3/180 /content/yolov5/dogs/download (10).jpg: 352x640 2 Dogs, Done. (0.242s)\n",
            "image 4/180 /content/yolov5/dogs/download (11).jpg: 416x640 4 Dogs, Done. (0.268s)\n",
            "image 5/180 /content/yolov5/dogs/download (12).jpg: 384x640 4 Dogs, Done. (0.260s)\n",
            "image 6/180 /content/yolov5/dogs/download (13).jpg: 384x640 1 Dog, Done. (0.239s)\n",
            "image 7/180 /content/yolov5/dogs/download (14).jpg: 384x640 1 Dog, Done. (0.249s)\n",
            "image 8/180 /content/yolov5/dogs/download (15).jpg: 384x640 3 Dogs, Done. (0.235s)\n",
            "image 9/180 /content/yolov5/dogs/download (16).jpg: 448x640 Done. (0.279s)\n",
            "image 10/180 /content/yolov5/dogs/download (17).jpg: 384x640 Done. (0.234s)\n",
            "image 11/180 /content/yolov5/dogs/download (18).jpg: 640x640 1 Dog, Done. (0.420s)\n",
            "image 12/180 /content/yolov5/dogs/download (19).jpg: 480x640 1 Dog, Done. (0.301s)\n",
            "image 13/180 /content/yolov5/dogs/download (2).jpg: 384x640 3 Dogs, Done. (0.235s)\n",
            "image 14/180 /content/yolov5/dogs/download (20).jpg: 640x448 1 Dog, Done. (0.296s)\n",
            "image 15/180 /content/yolov5/dogs/download (21).jpg: 480x640 1 Dog, Done. (0.311s)\n",
            "image 16/180 /content/yolov5/dogs/download (22).jpg: 640x480 1 Dog, Done. (0.334s)\n",
            "image 17/180 /content/yolov5/dogs/download (23).jpg: 480x640 1 Dog, Done. (0.312s)\n",
            "image 18/180 /content/yolov5/dogs/download (24).jpg: 480x640 1 Dog, Done. (0.313s)\n",
            "image 19/180 /content/yolov5/dogs/download (25).jpg: 448x640 1 Dog, Done. (0.279s)\n",
            "image 20/180 /content/yolov5/dogs/download (26).jpg: 480x640 1 Dog, Done. (0.298s)\n",
            "image 21/180 /content/yolov5/dogs/download (27).jpg: 640x640 1 Dog, Done. (0.418s)\n",
            "image 22/180 /content/yolov5/dogs/download (28).jpg: 448x640 1 Dog, Done. (0.298s)\n",
            "image 23/180 /content/yolov5/dogs/download (29).jpg: 448x640 1 Dog, Done. (0.301s)\n",
            "image 24/180 /content/yolov5/dogs/download (3).jpg: 352x640 2 Dogs, Done. (0.237s)\n",
            "image 25/180 /content/yolov5/dogs/download (30).jpg: 448x640 1 Dog, Done. (0.289s)\n",
            "image 26/180 /content/yolov5/dogs/download (31).jpg: 640x640 1 Dog, Done. (0.412s)\n",
            "image 27/180 /content/yolov5/dogs/download (32).jpg: 384x640 1 Dog, Done. (0.259s)\n",
            "image 28/180 /content/yolov5/dogs/download (33).jpg: 448x640 1 Dog, Done. (0.281s)\n",
            "image 29/180 /content/yolov5/dogs/download (34).jpg: 384x640 1 Dog, Done. (0.248s)\n",
            "image 30/180 /content/yolov5/dogs/download (35).jpg: 448x640 1 Dog, Done. (0.283s)\n",
            "image 31/180 /content/yolov5/dogs/download (4).jpg: 384x640 5 Dogs, Done. (0.266s)\n",
            "image 32/180 /content/yolov5/dogs/download (5).jpg: 448x640 1 Dog, Done. (0.286s)\n",
            "image 33/180 /content/yolov5/dogs/download (6).jpg: 384x640 Done. (0.240s)\n",
            "image 34/180 /content/yolov5/dogs/download (7).jpg: 384x640 1 Dog, Done. (0.252s)\n",
            "image 35/180 /content/yolov5/dogs/download (8).jpg: 448x640 2 Dogs, Done. (0.277s)\n",
            "image 36/180 /content/yolov5/dogs/download (9).jpg: 352x640 2 Dogs, Done. (0.220s)\n",
            "image 37/180 /content/yolov5/dogs/download.jpg: 480x640 7 Dogs, Done. (0.300s)\n",
            "image 38/180 /content/yolov5/dogs/images (1).jpg: 480x640 1 Dog, Done. (0.313s)\n",
            "image 39/180 /content/yolov5/dogs/images (10).jpg: 384x640 3 Dogs, Done. (0.246s)\n",
            "image 40/180 /content/yolov5/dogs/images (100).jpg: 416x640 1 Dog, Done. (0.260s)\n",
            "image 41/180 /content/yolov5/dogs/images (11).jpg: 352x640 3 Dogs, Done. (0.220s)\n",
            "image 42/180 /content/yolov5/dogs/images (12).jpg: 480x640 3 Dogs, Done. (0.303s)\n",
            "image 43/180 /content/yolov5/dogs/images (13).jpg: 384x640 4 Dogs, Done. (0.234s)\n",
            "image 44/180 /content/yolov5/dogs/images (14).jpg: 416x640 4 Dogs, Done. (0.262s)\n",
            "image 45/180 /content/yolov5/dogs/images (15).jpg: 384x640 2 Dogs, Done. (0.239s)\n",
            "image 46/180 /content/yolov5/dogs/images (16).jpg: 480x640 4 Dogs, Done. (0.311s)\n",
            "image 47/180 /content/yolov5/dogs/images (17).jpg: 416x640 3 Dogs, Done. (0.259s)\n",
            "image 48/180 /content/yolov5/dogs/images (18).jpg: 480x640 2 Dogs, Done. (0.302s)\n",
            "image 49/180 /content/yolov5/dogs/images (19).jpg: 384x640 3 Dogs, Done. (0.251s)\n",
            "image 50/180 /content/yolov5/dogs/images (2).jpg: 480x640 1 Dog, Done. (0.299s)\n",
            "image 51/180 /content/yolov5/dogs/images (20).jpg: 352x640 2 Dogs, Done. (0.221s)\n",
            "image 52/180 /content/yolov5/dogs/images (21).jpg: 384x640 2 Dogs, Done. (0.256s)\n",
            "image 53/180 /content/yolov5/dogs/images (22).jpg: 384x640 2 Dogs, Done. (0.250s)\n",
            "image 54/180 /content/yolov5/dogs/images (23).jpg: 320x640 2 Dogs, Done. (0.211s)\n",
            "image 55/180 /content/yolov5/dogs/images (24).jpg: 416x640 2 Dogs, Done. (0.268s)\n",
            "image 56/180 /content/yolov5/dogs/images (25).jpg: 480x640 Done. (0.295s)\n",
            "image 57/180 /content/yolov5/dogs/images (26).jpg: 480x640 1 Dog, Done. (0.322s)\n",
            "image 58/180 /content/yolov5/dogs/images (27).jpg: 384x640 5 Dogs, Done. (0.253s)\n",
            "image 59/180 /content/yolov5/dogs/images (28).jpg: 448x640 2 Dogs, Done. (0.278s)\n",
            "image 60/180 /content/yolov5/dogs/images (29).jpg: 384x640 3 Dogs, Done. (0.232s)\n",
            "image 61/180 /content/yolov5/dogs/images (3).jpg: 448x640 2 Dogs, Done. (0.284s)\n",
            "image 62/180 /content/yolov5/dogs/images (30).jpg: 384x640 7 Dogs, Done. (0.235s)\n",
            "image 63/180 /content/yolov5/dogs/images (31).jpg: 384x640 1 Dog, Done. (0.238s)\n",
            "image 64/180 /content/yolov5/dogs/images (32).jpg: 480x640 4 Dogs, Done. (0.296s)\n",
            "image 65/180 /content/yolov5/dogs/images (33).jpg: 416x640 3 Dogs, Done. (0.268s)\n",
            "image 66/180 /content/yolov5/dogs/images (34).jpg: 384x640 3 Dogs, Done. (0.245s)\n",
            "image 67/180 /content/yolov5/dogs/images (35).jpg: 384x640 4 Dogs, Done. (0.249s)\n",
            "image 68/180 /content/yolov5/dogs/images (36).jpg: 480x640 4 Dogs, Done. (0.310s)\n",
            "image 69/180 /content/yolov5/dogs/images (37).jpg: 448x640 3 Dogs, Done. (0.283s)\n",
            "image 70/180 /content/yolov5/dogs/images (38).jpg: 352x640 1 Dog, Done. (0.219s)\n",
            "image 71/180 /content/yolov5/dogs/images (39).jpg: 480x640 1 Dog, Done. (0.298s)\n",
            "image 72/180 /content/yolov5/dogs/images (4).jpg: 352x640 2 Dogs, Done. (0.222s)\n",
            "image 73/180 /content/yolov5/dogs/images (40).jpg: 288x640 5 Dogs, Done. (0.179s)\n",
            "image 74/180 /content/yolov5/dogs/images (41).jpg: 384x640 1 Dog, Done. (0.242s)\n",
            "image 75/180 /content/yolov5/dogs/images (42).jpg: 448x640 1 Dog, Done. (0.282s)\n",
            "image 76/180 /content/yolov5/dogs/images (43).jpg: 416x640 3 Dogs, Done. (0.273s)\n",
            "image 77/180 /content/yolov5/dogs/images (44).jpg: 384x640 4 Dogs, Done. (0.252s)\n",
            "image 78/180 /content/yolov5/dogs/images (45).jpg: 480x640 4 Dogs, Done. (0.311s)\n",
            "image 79/180 /content/yolov5/dogs/images (46).jpg: 352x640 2 Dogs, Done. (0.224s)\n",
            "image 80/180 /content/yolov5/dogs/images (47).jpg: 480x640 1 Dog, Done. (0.311s)\n",
            "image 81/180 /content/yolov5/dogs/images (48).jpg: 640x640 1 Dog, Done. (0.402s)\n",
            "image 82/180 /content/yolov5/dogs/images (49).jpg: 448x640 2 Dogs, Done. (0.280s)\n",
            "image 83/180 /content/yolov5/dogs/images (5).jpg: 352x640 2 Dogs, Done. (0.228s)\n",
            "image 84/180 /content/yolov5/dogs/images (50).jpg: 384x640 7 Dogs, Done. (0.242s)\n",
            "image 85/180 /content/yolov5/dogs/images (51).jpg: 384x640 2 Dogs, Done. (0.249s)\n",
            "image 86/180 /content/yolov5/dogs/images (52).jpg: 480x640 2 Dogs, Done. (0.295s)\n",
            "image 87/180 /content/yolov5/dogs/images (53).jpg: 320x640 3 Dogs, Done. (0.203s)\n",
            "image 88/180 /content/yolov5/dogs/images (54).jpg: 384x640 2 Dogs, Done. (0.244s)\n",
            "image 89/180 /content/yolov5/dogs/images (55).jpg: 352x640 2 Dogs, Done. (0.230s)\n",
            "image 90/180 /content/yolov5/dogs/images (56).jpg: 320x640 2 Dogs, Done. (0.216s)\n",
            "image 91/180 /content/yolov5/dogs/images (57).jpg: 384x640 4 Dogs, Done. (0.275s)\n",
            "image 92/180 /content/yolov5/dogs/images (58).jpg: 480x640 1 Dog, Done. (0.311s)\n",
            "image 93/180 /content/yolov5/dogs/images (59).jpg: 384x640 2 Dogs, Done. (0.319s)\n",
            "image 94/180 /content/yolov5/dogs/images (6).jpg: 448x640 2 Dogs, Done. (0.450s)\n",
            "image 95/180 /content/yolov5/dogs/images (60).jpg: 448x640 2 Dogs, Done. (0.467s)\n",
            "image 96/180 /content/yolov5/dogs/images (61).jpg: 352x640 2 Dogs, Done. (0.463s)\n",
            "image 97/180 /content/yolov5/dogs/images (62).jpg: 480x640 1 Dog, Done. (0.349s)\n",
            "image 98/180 /content/yolov5/dogs/images (63).jpg: 416x640 3 Dogs, Done. (0.461s)\n",
            "image 99/180 /content/yolov5/dogs/images (64).jpg: 416x640 1 Dog, Done. (0.452s)\n",
            "image 100/180 /content/yolov5/dogs/images (65).jpg: 448x640 1 Dog, Done. (0.551s)\n",
            "image 101/180 /content/yolov5/dogs/images (66).jpg: 320x640 2 Dogs, Done. (0.526s)\n",
            "image 102/180 /content/yolov5/dogs/images (67).jpg: 384x640 1 Dog, Done. (0.463s)\n",
            "image 103/180 /content/yolov5/dogs/images (68).jpg: 384x640 1 Dog, Done. (0.256s)\n",
            "image 104/180 /content/yolov5/dogs/images (69).jpg: 384x640 1 Dog, Done. (0.243s)\n",
            "image 105/180 /content/yolov5/dogs/images (7).jpg: 384x640 3 Dogs, Done. (0.241s)\n",
            "image 106/180 /content/yolov5/dogs/images (70).jpg: 480x640 4 Dogs, Done. (0.300s)\n",
            "image 107/180 /content/yolov5/dogs/images (71).jpg: 416x640 2 Dogs, Done. (0.280s)\n",
            "image 108/180 /content/yolov5/dogs/images (72).jpg: 384x640 3 Dogs, Done. (0.250s)\n",
            "image 109/180 /content/yolov5/dogs/images (73).jpg: 384x640 2 Dogs, Done. (0.246s)\n",
            "image 110/180 /content/yolov5/dogs/images (74).jpg: 448x640 1 Dog, Done. (0.292s)\n",
            "image 111/180 /content/yolov5/dogs/images (75).jpg: 480x640 1 Dog, Done. (0.315s)\n",
            "image 112/180 /content/yolov5/dogs/images (76).jpg: 384x640 1 Dog, Done. (0.242s)\n",
            "image 113/180 /content/yolov5/dogs/images (77).jpg: 416x640 2 Dogs, Done. (0.267s)\n",
            "image 114/180 /content/yolov5/dogs/images (78).jpg: 448x640 2 Dogs, Done. (0.293s)\n",
            "image 115/180 /content/yolov5/dogs/images (79).jpg: 384x640 2 Dogs, Done. (0.247s)\n",
            "image 116/180 /content/yolov5/dogs/images (8).jpg: 480x640 2 Dogs, Done. (0.306s)\n",
            "image 117/180 /content/yolov5/dogs/images (80).jpg: 384x640 1 Dog, Done. (0.249s)\n",
            "image 118/180 /content/yolov5/dogs/images (81).jpg: 448x640 2 Dogs, Done. (0.305s)\n",
            "image 119/180 /content/yolov5/dogs/images (82).jpg: 480x640 1 Dog, Done. (0.311s)\n",
            "image 120/180 /content/yolov5/dogs/images (83).jpg: 480x640 1 Dog, Done. (0.314s)\n",
            "image 121/180 /content/yolov5/dogs/images (84).jpg: 384x640 Done. (0.259s)\n",
            "image 122/180 /content/yolov5/dogs/images (85).jpg: 384x640 1 Dog, Done. (0.241s)\n",
            "image 123/180 /content/yolov5/dogs/images (86).jpg: 384x640 5 Dogs, Done. (0.246s)\n",
            "image 124/180 /content/yolov5/dogs/images (87).jpg: 640x640 1 Dog, Done. (0.398s)\n",
            "image 125/180 /content/yolov5/dogs/images (88).jpg: 480x640 1 Dog, Done. (0.318s)\n",
            "image 126/180 /content/yolov5/dogs/images (89).jpg: 384x640 2 Dogs, Done. (0.241s)\n",
            "image 127/180 /content/yolov5/dogs/images (9).jpg: 352x640 1 Dog, Done. (0.225s)\n",
            "image 128/180 /content/yolov5/dogs/images (90).jpg: 448x640 3 Dogs, Done. (0.291s)\n",
            "image 129/180 /content/yolov5/dogs/images (91).jpg: 448x640 1 Dog, Done. (0.293s)\n",
            "image 130/180 /content/yolov5/dogs/images (92).jpg: 480x640 1 Dog, Done. (0.303s)\n",
            "image 131/180 /content/yolov5/dogs/images (93).jpg: 544x640 1 Dog, Done. (0.355s)\n",
            "image 132/180 /content/yolov5/dogs/images (94).jpg: 448x640 1 Dog, Done. (0.307s)\n",
            "image 133/180 /content/yolov5/dogs/images (95).jpg: 384x640 1 Dog, Done. (0.253s)\n",
            "image 134/180 /content/yolov5/dogs/images (96).jpg: 640x640 1 Dog, Done. (0.421s)\n",
            "image 135/180 /content/yolov5/dogs/images (97).jpg: 384x640 2 Dogs, Done. (0.243s)\n",
            "image 136/180 /content/yolov5/dogs/images (98).jpg: 448x640 1 Dog, Done. (0.299s)\n",
            "image 137/180 /content/yolov5/dogs/images (99).jpg: 448x640 1 Dog, Done. (0.291s)\n",
            "image 138/180 /content/yolov5/dogs/images - 2022-04-24T173026.205.jpg: 480x640 1 Dog, Done. (0.324s)\n",
            "image 139/180 /content/yolov5/dogs/images - 2022-04-24T173039.522.jpg: 640x640 Done. (0.419s)\n",
            "image 140/180 /content/yolov5/dogs/images - 2022-04-24T173043.365.jpg: 640x448 1 Dog, Done. (0.289s)\n",
            "image 141/180 /content/yolov5/dogs/images - 2022-04-24T173046.987.jpg: 640x384 1 Dog, Done. (0.263s)\n",
            "image 142/180 /content/yolov5/dogs/images - 2022-04-24T173051.067.jpg: 480x640 2 Dogs, Done. (0.334s)\n",
            "image 143/180 /content/yolov5/dogs/images - 2022-04-24T173055.552.jpg: 640x640 1 Dog, Done. (0.425s)\n",
            "image 144/180 /content/yolov5/dogs/images - 2022-04-24T173101.027.jpg: 480x640 1 Dog, Done. (0.324s)\n",
            "image 145/180 /content/yolov5/dogs/images - 2022-04-24T173105.378.jpg: 576x640 1 Dog, Done. (0.380s)\n",
            "image 146/180 /content/yolov5/dogs/images - 2022-04-24T173111.254.jpg: 384x640 2 Dogs, Done. (0.250s)\n",
            "image 147/180 /content/yolov5/dogs/images - 2022-04-24T173114.353.jpg: 320x640 1 Dog, Done. (0.217s)\n",
            "image 148/180 /content/yolov5/dogs/images - 2022-04-24T173120.592.jpg: 448x640 1 Dog, Done. (0.302s)\n",
            "image 149/180 /content/yolov5/dogs/images - 2022-04-24T173136.045.jpg: 480x640 1 Dog, Done. (0.303s)\n",
            "image 150/180 /content/yolov5/dogs/images - 2022-04-24T173146.789.jpg: 544x640 1 Dog, Done. (0.370s)\n",
            "image 151/180 /content/yolov5/dogs/images - 2022-04-24T173150.635.jpg: 320x640 2 Dogs, Done. (0.215s)\n",
            "image 152/180 /content/yolov5/dogs/images - 2022-04-24T173157.348.jpg: 480x640 1 Dog, Done. (0.319s)\n",
            "image 153/180 /content/yolov5/dogs/images - 2022-04-24T173207.289.jpg: 352x640 1 Dog, Done. (0.232s)\n",
            "image 154/180 /content/yolov5/dogs/images - 2022-04-24T173212.852.jpg: 448x640 1 Dog, Done. (0.311s)\n",
            "image 155/180 /content/yolov5/dogs/images - 2022-04-24T173348.862.jpg: 608x640 1 Dog, Done. (0.393s)\n",
            "image 156/180 /content/yolov5/dogs/images - 2022-04-24T173404.112.jpg: 384x640 1 Dog, Done. (0.273s)\n",
            "image 157/180 /content/yolov5/dogs/images - 2022-04-24T173506.535.jpg: 288x640 4 Dogs, Done. (0.196s)\n",
            "image 158/180 /content/yolov5/dogs/images - 2022-04-24T173514.794.jpg: 384x640 4 Dogs, Done. (0.255s)\n",
            "image 159/180 /content/yolov5/dogs/images - 2022-04-24T173517.965.jpg: 384x640 2 Dogs, Done. (0.247s)\n",
            "image 160/180 /content/yolov5/dogs/images - 2022-04-24T173524.145.jpg: 320x640 1 Dog, Done. (0.212s)\n",
            "image 161/180 /content/yolov5/dogs/images - 2022-04-24T173529.322.jpg: 384x640 1 Dog, Done. (0.246s)\n",
            "image 162/180 /content/yolov5/dogs/images - 2022-04-24T173537.212.jpg: 384x640 1 Dog, Done. (0.250s)\n",
            "image 163/180 /content/yolov5/dogs/images - 2022-04-24T173541.020.jpg: 384x640 1 Dog, Done. (0.239s)\n",
            "image 164/180 /content/yolov5/dogs/images - 2022-04-24T173549.872.jpg: 448x640 1 Dog, Done. (0.290s)\n",
            "image 165/180 /content/yolov5/dogs/images - 2022-04-24T173601.634.jpg: 384x640 1 Dog, Done. (0.257s)\n",
            "image 166/180 /content/yolov5/dogs/images - 2022-04-24T173610.609.jpg: 480x640 1 Dog, Done. (0.304s)\n",
            "image 167/180 /content/yolov5/dogs/images - 2022-04-24T173613.818.jpg: 448x640 3 Dogs, Done. (0.287s)\n",
            "image 168/180 /content/yolov5/dogs/images - 2022-04-24T173624.282.jpg: 384x640 Done. (0.250s)\n",
            "image 169/180 /content/yolov5/dogs/images - 2022-04-24T173628.619.jpg: 480x640 2 Dogs, Done. (0.331s)\n",
            "image 170/180 /content/yolov5/dogs/images - 2022-04-24T173637.499.jpg: 416x640 1 Dog, Done. (0.287s)\n",
            "image 171/180 /content/yolov5/dogs/images - 2022-04-24T173642.392.jpg: 384x640 1 Dog, Done. (0.256s)\n",
            "image 172/180 /content/yolov5/dogs/images - 2022-04-24T173646.052.jpg: 384x640 4 Dogs, Done. (0.292s)\n",
            "image 173/180 /content/yolov5/dogs/images - 2022-04-24T173651.596.jpg: 480x640 1 Dog, Done. (0.341s)\n",
            "image 174/180 /content/yolov5/dogs/images - 2022-04-24T173655.953.jpg: 640x576 1 Dog, Done. (0.397s)\n",
            "image 175/180 /content/yolov5/dogs/images - 2022-04-24T173659.642.jpg: 384x640 1 Dog, Done. (0.307s)\n",
            "image 176/180 /content/yolov5/dogs/images - 2022-04-24T173706.435.jpg: 384x640 2 Dogs, Done. (0.270s)\n",
            "image 177/180 /content/yolov5/dogs/images - 2022-04-24T173718.721.jpg: 448x640 2 Dogs, Done. (0.305s)\n",
            "image 178/180 /content/yolov5/dogs/images - 2022-04-24T173736.147.jpg: 416x640 2 Dogs, Done. (0.287s)\n",
            "image 179/180 /content/yolov5/dogs/images - 2022-04-24T173748.352.jpg: 384x640 4 Dogs, Done. (0.271s)\n",
            "image 180/180 /content/yolov5/dogs/images.jpg: 384x640 2 Dogs, Done. (0.267s)\n",
            "Speed: 1.3ms pre-process, 288.9ms inference, 0.9ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp13\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exp13 --> chunk5\n",
        "exp12 --> chunk4"
      ],
      "metadata": {
        "id": "80wiasZk3psX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!zip /content/yolov5/runs/detect/exp13 -r /content/yolov5/runs/detect/exp13"
      ],
      "metadata": {
        "id": "-ZDIWZWG2M94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ------------- the weights of chunk5 are the best! :))\n"
      ],
      "metadata": {
        "id": "r939iV0x5Zvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# working on fypdataset3"
      ],
      "metadata": {
        "id": "fvAk18eHQ81r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/fypdataset3.zip"
      ],
      "metadata": {
        "id": "wk9XPv5QRCM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* we got some png images, so changing them into jpg:"
      ],
      "metadata": {
        "id": "Yjpy_HBcRXao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "image_path = \"/content/fypdataset3/images\"\n",
        "\n",
        "for i in os.listdir(image_path):\n",
        "    base = os.path.splitext(i)[0]\n",
        "    os.rename(\"/content/fypdataset3/images/\"+i, \"/content/fypdataset3/images/\" +base + '.jpg')\n"
      ],
      "metadata": {
        "id": "kOQ-FvnCQ_Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* train, val, test split:"
      ],
      "metadata": {
        "id": "k9awV1qfSnzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "annotation_root=\"/content/fypdataset3/labels\"\n",
        "root_path=\"/content/fypdataset3\"\n",
        "\n",
        "data = {}\n",
        "path_to_annotation_files = []\n",
        "path_to_image_files = []\n",
        "for i in os.listdir(annotation_root):\n",
        "  annotation_path = os.path.join(annotation_root, i)\n",
        "  y= re.split(\".txt\",i)\n",
        "  image_path = os.path.join(root_path, f\"images/{y[0]}.jpg\")\n",
        "  path_to_annotation_files.append(annotation_path)\n",
        "  path_to_image_files.append(image_path)\n",
        "\n",
        "\n",
        "print(\"no of images: \", len(path_to_annotation_files))\n",
        "print(\"no of labels: \", len(path_to_image_files))\n",
        "\n",
        "train_images, val_images, train_annotations, val_annotations= train_test_split(path_to_image_files, path_to_annotation_files,\n",
        "                                                                               test_size=0.2, random_state=1)\n",
        "\n",
        "val_images, test_images, val_annotations, test_annotations=train_test_split(val_images, val_annotations,\n",
        "                                                                           test_size=0.5, random_state=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "SxsTBoDaSqJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def moves_files_to_folder(list_of_files, destination_folder):\n",
        "  for f in list_of_files:\n",
        "    try:\n",
        "      shutil.copy(f, destination_folder)\n",
        "    except:\n",
        "      print(f)\n",
        "      assert False\n",
        "\n",
        "moves_files_to_folder(train_images, \"/content/fypdataset3/images/training\")\n",
        "moves_files_to_folder(val_images, \"/content/fypdataset3/images/validation\")\n",
        "moves_files_to_folder(test_images, \"/content/fypdataset3/images/testing\")\n",
        "moves_files_to_folder(train_annotations, \"/content/fypdataset3/labels/training\")\n",
        "moves_files_to_folder(val_annotations, \"/content/fypdataset3/labels/validation\")\n",
        "moves_files_to_folder(test_annotations, \"/content/fypdataset3/labels/testing\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PEZVTskfSvG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree(\"/content/fypdataset3/labels1\")\n",
        "shutil.rmtree(\"/content/fypdataset3/images1\")"
      ],
      "metadata": {
        "id": "GiWiFZm9S-OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "6mEzsJWnTEep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* train, detect and validate your data :))"
      ],
      "metadata": {
        "id": "cQLCn130TSi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 12 --epochs 10  --data /content/yolov5/data/custom_dataset.yaml --weights /content/best.pt --cache"
      ],
      "metadata": {
        "id": "BCc_sCbdTHTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 100th epoch:"
      ],
      "metadata": {
        "id": "aAGh7GwMOAMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/fyp/fypdataset.zip\n",
        "!unzip /content/drive/MyDrive/fyp/fypdataset2.zip"
      ],
      "metadata": {
        "id": "0VyjWq0zODYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmyhxkdoOsB3",
        "outputId": "8458ae0f-b0bb-4ead-c003-0265348d2972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 13289, done.\u001b[K\n",
            "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 13289 (delta 53), reused 72 (delta 34), pack-reused 13188\u001b[K\n",
            "Receiving objects: 100% (13289/13289), 12.57 MiB | 23.93 MiB/s, done.\n",
            "Resolving deltas: 100% (9108/9108), done.\n",
            "/content/yolov5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (7.1.2)\n",
            "Collecting PyYAML>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.12.0+cu113)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (0.13.0+cu113)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (4.64.0)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (3.17.3)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (2.8.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 22)) (1.3.5)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (0.11.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 37)) (5.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 38)) (5.4.8)\n",
            "Collecting thop>=0.1.1\n",
            "  Downloading thop-0.1.1.post2207130030-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r requirements.txt (line 12)) (4.1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1->-r requirements.txt (line 15)) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.47.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.35.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 22)) (2022.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 18)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 18)) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 18)) (3.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (4.4.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r requirements.txt (line 37)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->-r requirements.txt (line 37)) (0.7.0)\n",
            "Installing collected packages: thop, PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 thop-0.1.1.post2207130030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.call([\"/content/yolov5/data/scripts/download_weights.sh\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMaZPttWQbdU",
        "outputId": "62b0a6d0-2e93-4833-ac9b-d106cf8d50af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --cfg /content/yolov5/models/yolov5m.yaml --hyp /content/yolov5/data/hyps/hyp.scratch-med.yaml --batch 32 --epochs 10 --data /content/yolov5/data/custom_dataset.yaml --weights /content/drive/MyDrive/FYP/exp/content/yolov5/runs/train/exp3/weights/best.pt --workers 24"
      ],
      "metadata": {
        "id": "guhivGnGQd7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e857bf-3836-4d48-86c1-9f1af7ed9b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/drive/MyDrive/FYP/exp/content/yolov5/runs/train/exp3/weights/best.pt, cfg=/content/yolov5/models/yolov5m.yaml, data=/content/yolov5/data/custom_dataset.yaml, hyp=/content/yolov5/data/hyps/hyp.scratch-med.yaml, epochs=10, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=24, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (Docker image), for updates see https://github.com/ultralytics/yolov5\n",
            "YOLOv5 ðŸš€ v6.1-344-g0e165c5 Python-3.7.13 torch-1.12.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.1, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
            "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
            "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
            "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
            "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
            "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
            "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
            "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
            "  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
            "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
            " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
            " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
            " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
            " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
            " 24      [17, 20, 23]  1     36369  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
            "YOLOv5m summary: 369 layers, 20883441 parameters, 20883441 gradients, 48.3 GFLOPs\n",
            "\n",
            "Transferred 480/481 items from /content/drive/MyDrive/FYP/exp/content/yolov5/runs/train/exp3/weights/best.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 79 weight (no decay), 82 weight, 82 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/fypdataset4/labels.cache' images and labels... 10149 found, 8 missing, 0 empty, 3 corrupt: 100% 10160/10160 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset4/images/10026.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset4/images/8470.jpg: ignoring corrupt image/label: invalid image format GIF\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset4/images/8577.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset4/images/8646.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset4/images/9297.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset4/images/9565.jpg: ignoring corrupt image/label: invalid image format GIF\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset4/images/9778.jpg: ignoring corrupt image/label: invalid image format GIF\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset4/images/9815.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset4/images/n02089973_1763.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset4/images/n02089973_2054.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/content/fypdataset/labels/validation.cache' images and labels... 407 found, 0 missing, 0 empty, 0 corrupt: 100% 407/407 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING: /content/content/fypdataset/images/validation/9297.jpg: corrupt JPEG restored and saved\n",
            "Plotting labels to runs/train/exp4/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.56 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp4\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/9     12.1G    0.0179  0.008679 0.0008283        41       640: 100% 318/318 [05:25<00:00,  1.02s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.09it/s]\n",
            "                 all        407        444      0.956       0.97       0.99      0.757\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/9     10.8G   0.02142  0.009351   0.00148        40       640: 100% 318/318 [05:20<00:00,  1.01s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.15it/s]\n",
            "                 all        407        444      0.972      0.969      0.988      0.766\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/9     10.8G   0.02182  0.009677  0.001634        47       640: 100% 318/318 [05:17<00:00,  1.00it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.16it/s]\n",
            "                 all        407        444      0.926      0.909      0.956       0.71\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/9     10.8G   0.02311   0.01013  0.001806        42       640: 100% 318/318 [05:33<00:00,  1.05s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.03it/s]\n",
            "                 all        407        444      0.949      0.944      0.964      0.708\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/9     10.8G   0.02352   0.01039  0.002039        44       640: 100% 318/318 [05:41<00:00,  1.08s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.14it/s]\n",
            "                 all        407        444      0.936      0.958      0.978      0.694\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/9     10.8G   0.02282   0.01009  0.001948        31       640: 100% 318/318 [05:25<00:00,  1.02s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.04it/s]\n",
            "                 all        407        444      0.952      0.949      0.978      0.714\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/9     10.8G   0.02239  0.009966  0.001838        42       640: 100% 318/318 [05:17<00:00,  1.00it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.12it/s]\n",
            "                 all        407        444      0.946      0.954       0.98      0.732\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       7/9     10.8G   0.02168  0.009826  0.001672        47       640: 100% 318/318 [05:22<00:00,  1.02s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.13it/s]\n",
            "                 all        407        444      0.962      0.956      0.983      0.764\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       8/9     10.8G   0.02127  0.009601  0.001581        39       640: 100% 318/318 [05:21<00:00,  1.01s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.19it/s]\n",
            "                 all        407        444      0.971      0.951      0.984      0.749\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       9/9     10.8G   0.02109  0.009605  0.001462        38       640: 100% 318/318 [05:15<00:00,  1.01it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.14it/s]\n",
            "                 all        407        444      0.947      0.979      0.989      0.766\n",
            "\n",
            "10 epochs completed in 0.921 hours.\n",
            "Optimizer stripped from runs/train/exp4/weights/last.pt, 42.2MB\n",
            "Optimizer stripped from runs/train/exp4/weights/best.pt, 42.2MB\n",
            "\n",
            "Validating runs/train/exp4/weights/best.pt...\n",
            "Fusing layers... \n",
            "YOLOv5m summary: 290 layers, 20865057 parameters, 0 gradients, 47.9 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:07<00:00,  1.00s/it]\n",
            "                 all        407        444      0.947      0.979      0.989      0.767\n",
            "                 Dog        407        303       0.96      0.997      0.991      0.884\n",
            "                 Cat        407         75      0.934      0.939      0.977      0.694\n",
            "               Horse        407         17      0.956          1      0.995      0.855\n",
            "               Human        407         49      0.938       0.98      0.991      0.634\n",
            "Results saved to \u001b[1mruns/train/exp4\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/exp4 -r /content/yolov5/runs/train/exp4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 120 epochs,, haha"
      ],
      "metadata": {
        "id": "BVdFqjabPoIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/yolov5/runs/train/exp4/weights/best.pt --img 640 --conf 0.5 --source /content/drive/MyDrive/FYP/test_videos/cat_dog.mp4"
      ],
      "metadata": {
        "id": "NqRGvNnVQDm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/drive/MyDrive/FYP/exp/content/yolov5/runs/train/exp3/weights/best.pt --img 640 --conf 0.5 --source /content/drive/MyDrive/FYP/test_videos/cat_dog.mp4"
      ],
      "metadata": {
        "id": "cnKh-tPNjotr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1FScWB8wATq5bdN6x9H_BZOVxE9KipENo",
      "authorship_tag": "ABX9TyOjGmyZS5940ofDzJFquY/g",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}